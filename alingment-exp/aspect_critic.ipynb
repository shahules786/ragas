{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f708066d-2e7d-46c1-81db-7d5f526c6f74",
   "metadata": {},
   "source": [
    "## LLM\n",
    "- the idea to experiment the same methods on multiple LLMs to make sure the result are LLM agnostic, ideally. Practically measure the std b/w different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7236a707-306b-403a-ba19-916c9ad82491",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd3cabd-8ec6-4f55-9bd3-68b38ad70f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30b7e92-57e7-482e-b1ce-b55a259d2d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ragas/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "llm_4o = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "llm_4o_mini = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98778a7-e8c9-42c8-9f96-b70c30603484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(embeddings=embeddings) # any langchain Embeddings instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279ad431-b006-464e-b9b6-5e2bcf6d71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "config = {\n",
    "    \"credentials_profile_name\": \"default\",  # E.g \"default\"\n",
    "    \"region_name\": \"us-east-1\",  # E.g. \"us-east-1\"\n",
    "    \"llm\": \"anthropic.claude-3-haiku-20240307-v1:0\",  # E.g \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "}\n",
    "\n",
    "bedrock_llm = ChatBedrockConverse(\n",
    "    credentials_profile_name=config[\"credentials_profile_name\"],\n",
    "    region_name=config[\"region_name\"],\n",
    "    base_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "    model=config[\"llm\"],\n",
    ")\n",
    "\n",
    "bedrock_llm = LangchainLLMWrapper(bedrock_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c93aed-a688-4ac9-84f5-fad2e81988bc",
   "metadata": {},
   "source": [
    "## tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f9ae5f85-23a2-4e82-ac4c-e18c9290c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]= \"Alignment\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02887fbb-4cda-4921-8dd3-7ce72e827f5f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "9308cc73-785a-4418-b53b-4f10801f4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics._aspect_critic import AspectCriticWithReference\n",
    "from ragas import EvaluationDataset\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "11a351b5-1893-41bb-a3ff-a27e4e4ca744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_eval_dataset(path,postive_clas=0.6, seed=42):\n",
    "    samples = []\n",
    "    np.random.seed(seed)\n",
    "    dataset = Dataset.from_json(path)\n",
    "    num = int(len(dataset)*postive_clas)\n",
    "    positive_sample_indices = np.random.randint(0,50,num).tolist()\n",
    "    negative_sample_indices = [i for i in range(len(dataset)) if i not in positive_sample_indices]\n",
    "    for i,row in enumerate(dataset):\n",
    "        dic = {\n",
    "                \"user_input\":row[\"user_input\"],\n",
    "                \"reference\": row[\"reference\"],\n",
    "                                \n",
    "            }\n",
    "        if i in positive_sample_indices:\n",
    "            dic.update(\n",
    "                {\n",
    "                    \"response\": row[\"response\"],\n",
    "                    \"target\": 1\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "             dic.update(\n",
    "                {\n",
    "                    \"response\": row[\"errored_response\"],\n",
    "                    \"target\": 0\n",
    "                }\n",
    "            )\n",
    "        samples.append(dic)\n",
    "    return Dataset.from_list(samples)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "bbdf481f-7416-4f91-ab46-6341ede26387",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_eval_dataset(\"datasets/dataset_v4.json\",postive_clas=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7198b863-3a41-4f76-ad64-ba617dee3b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "2b9a48b5-1a07-461c-9091-ef1c02354e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "98558ed5-f4ac-421b-8d02-b848457c4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d53484-e1ab-40c7-97cd-7d645b05f229",
   "metadata": {},
   "source": [
    "## Aspect Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "59667893-0d0e-4e6d-aac7-261da9c64c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_examples(n=3):\n",
    "    from ragas.metrics._aspect_critic import AspectCriticInputWithReference,AspectCriticOutputWithReference\n",
    "    examples = [(\n",
    "    AspectCriticInputWithReference(\n",
    "        user_input=\"What is the main ingredient in traditional Japanese miso soup?\",\n",
    "        response=\"The main ingredient in traditional Japanese miso soup is soybeans.\",\n",
    "        reference=\"The main ingredient in traditional Japanese miso soup is miso paste, which is made from fermented soybeans.\",\n",
    "        criteria=\"Does the response accurately convey the main ingredient of the soup?\",\n",
    "    ),\n",
    "    AspectCriticOutputWithReference(\n",
    "        reason=\"The response mentions soybeans, which is part of the ingredient but does not fully capture the main ingredient, miso paste.\",\n",
    "        verdict=0,\n",
    "    ),\n",
    "),\n",
    "     (\n",
    "    AspectCriticInputWithReference(\n",
    "        user_input=\"When was the Declaration of Independence signed?\",\n",
    "        response=\"The Declaration of Independence was signed in 1776.\",\n",
    "        reference=\"The Declaration of Independence was signed on July 4, 1776.\",\n",
    "        criteria=\"Is the response factually complete, including both month and year?\",\n",
    "    ),\n",
    "    AspectCriticOutputWithReference(\n",
    "        reason=\"The response provides the correct year but omits the specific date, July 4, making it incomplete.\",\n",
    "        verdict=0,\n",
    "    ),\n",
    "),\n",
    "     (\n",
    "    AspectCriticInputWithReference(\n",
    "        user_input=\"What is the main ingredient in traditional Japanese miso soup?\",\n",
    "        response=\"The main ingredient in traditional Japanese miso soup is soybeans.\",\n",
    "        reference=\"The main ingredient in traditional Japanese miso soup is miso paste, which is made from fermented soybeans.\",\n",
    "        criteria=\"Does the response accurately convey the main ingredient of the soup?\",\n",
    "    ),\n",
    "    AspectCriticOutputWithReference(\n",
    "        reason=\"The response mentions soybeans, which is part of the ingredient but does not fully capture the main ingredient, miso paste.\",\n",
    "        verdict=0,\n",
    "    ),\n",
    ")]\n",
    "    return examples[:n]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "7df08839-9dc0-47ce-9f8d-81cdcf5407a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"Please evaluate the provided responses based on their accuracy and relevance to the user inputs and references. Assign a verdict of 1 for responses that align closely with the reference, even if they contain some additional context or elaboration, and 0 for incorrect responses that contain factual inaccuracies or significant deviations from the reference. Ensure that all critical details from the reference, including specific phrases or terms essential for accuracy, are included in the response to avoid missing key information that could affect the verdict. Provide a brief reason for your verdict, highlighting both strengths and weaknesses in the responses, and ensure that the evaluation remains concise, focusing on core factual accuracy and relevance to the reference. For guidance, examples of significant deviations include discrepancies in key dates or misinterpretations of critical facts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "fb75bea8-5fcc-4439-afae-b94123b860ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = AspectCriticWithReference(name=\"answer correctness\",\n",
    "                      definition=new_prompt,dynamic_retrevial=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "1e4961eb-719d-45d5-8eec-f3035b5fa861",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.single_turn_prompt.examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "8f68a003-8a6b-40a0-ac67-74f6f0391ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.llm = llm_4o_mini\n",
    "critic.embedding_model = langchain_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "73c5e53b-6e95-4682-8356-6bfcd2e31c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "eb5de705-5de0-46c8-b94e-ae9fa4472e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████| 50/50 [00:16<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(dataset=eval_dataset[:],metrics=[critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8363f5ed-2bdc-4476-9af4-614e212bab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "aff33001-0e8d-4f56-b34f-3fa2a9a729e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>answer correctness</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did the invention of the wheel impact anci...</td>\n",
       "      <td># The Revolutionary Impact of the Wheel on Anc...</td>\n",
       "      <td>The invention of the wheel was a pivotal momen...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did the discovery of fire impact early hum...</td>\n",
       "      <td># The Transformative Power of Fire in Early Hu...</td>\n",
       "      <td>The discovery of fire was a pivotal moment in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What were the major impacts of the Agricultura...</td>\n",
       "      <td># The Transformative Impacts of the Agricultur...</td>\n",
       "      <td>The Agricultural Revolution, which began aroun...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What were the key events and consequences of t...</td>\n",
       "      <td># The Fall of Constantinople: A Turning Point ...</td>\n",
       "      <td>The Fall of Constantinople occurred on May 29,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How did the birth of democracy in Athens shape...</td>\n",
       "      <td># The Birth of Democracy in Athens: A Revoluti...</td>\n",
       "      <td>The birth of democracy in Athens, around the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How did the invention of the wheel impact anci...   \n",
       "1  How did the discovery of fire impact early hum...   \n",
       "2  What were the major impacts of the Agricultura...   \n",
       "3  What were the key events and consequences of t...   \n",
       "4  How did the birth of democracy in Athens shape...   \n",
       "\n",
       "                                            response  \\\n",
       "0  # The Revolutionary Impact of the Wheel on Anc...   \n",
       "1  # The Transformative Power of Fire in Early Hu...   \n",
       "2  # The Transformative Impacts of the Agricultur...   \n",
       "3  # The Fall of Constantinople: A Turning Point ...   \n",
       "4  # The Birth of Democracy in Athens: A Revoluti...   \n",
       "\n",
       "                                           reference  answer correctness  \\\n",
       "0  The invention of the wheel was a pivotal momen...                   1   \n",
       "1  The discovery of fire was a pivotal moment in ...                   1   \n",
       "2  The Agricultural Revolution, which began aroun...                   1   \n",
       "3  The Fall of Constantinople occurred on May 29,...                   0   \n",
       "4  The birth of democracy in Athens, around the 5...                   0   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"] = y_true\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "26e9c252-59df-47cd-a6ea-7e644fcb32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df[\"answer correctness\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f3ac2-2a0c-494f-af1f-906ee1ca26ec",
   "metadata": {},
   "source": [
    "## evaluate the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "e5ca7bfa-5ed2-46b5-ae42-84d35c6a6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "a831ca75-9e90-4d29-93c0-349779acbb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.8301886792452831\n",
      "precision 0.7096774193548387\n",
      "recall 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"f1\",f1_score(y_true,y_pred))\n",
    "print(\"precision\",precision_score(y_true,y_pred))\n",
    "print(\"recall\",recall_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858363f2-fb75-4094-8fa9-66a2c6de48f8",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c2f2e7-5c18-45a6-bbbb-aeac0f6f69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_indices(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to find the indices of TP, FP, FN, and TN.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (list or np.array): Ground truth binary labels (0 or 1).\n",
    "    y_pred (list or np.array): Predicted binary labels (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing lists of indices for TP, FP, FN, and TN.\n",
    "    \"\"\"\n",
    "    tp = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true == 1 and pred == 1]\n",
    "    fp = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true == 0 and pred == 1]\n",
    "    fn = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true == 1 and pred == 0]\n",
    "    tn = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true == 0 and pred == 0]\n",
    "\n",
    "    return {\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TN': tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62fe01f5-0034-47fa-80e7-bb8449a67981",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = get_confusion_indices(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7cda7720-2e5f-4664-9a9c-f1680d6d4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "def serialize_for_json(data: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Convert custom objects into a JSON-serializable format.\n",
    "\n",
    "    Parameters:\n",
    "    data (Any): The data to be converted, which may contain custom objects.\n",
    "\n",
    "    Returns:\n",
    "    Any: A JSON-serializable version of the input data.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: serialize_for_json(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [serialize_for_json(item) for item in data]\n",
    "    elif hasattr(data, \"__dict__\"):\n",
    "        # Convert custom objects by serializing their attributes (assumes they use __dict__)\n",
    "        return serialize_for_json(data.__dict__)\n",
    "    else:\n",
    "        return data  # Return the data as-is if it is already JSON-serializable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceae867d-eeec-4a9e-9782-7c01b9311457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "samples = defaultdict(list)\n",
    "traces = result.traces\n",
    "for qdrant,indices in confusion_matrix.items():\n",
    "    for idx in indices:\n",
    "        samples[qdrant].append(serialize_for_json(traces[idx]['answer correctness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20a3e27b-1855-4797-a0a4-ffe336885501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 7, 1, 19]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(val) for val in samples.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f4be615-391e-4989-b57b-666e68b06b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"datasets/dataset_v4_training_aspect_critic_annotated.json\",\"w\") as file:\n",
    "    json.dump(samples,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73751ced-51a3-42aa-8412-bea1744e4c6d",
   "metadata": {},
   "source": [
    "## Embed and store samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc95d0d7-0fbe-47be-9043-34f893d19563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "samples = json.load(open(\"datasets/dataset_v4_training_aspect_critic_annotated.json\"))\n",
    "                     \n",
    "# embeddings.aembed_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "420863f3-c3ce-41bc-bb8c-1c4583810d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(num_samples=20,seed=42):\n",
    "    tp = samples[\"TP\"]\n",
    "    tn = samples[\"TN\"]\n",
    "    fp = samples['FP']\n",
    "    fn = samples['FN']\n",
    "    all_samples = tp + tn + fp + fn\n",
    "    final_samples = []\n",
    "    for sample in all_samples:\n",
    "        if sample in tp:\n",
    "            qdrant = 'TP'\n",
    "        elif sample in fp:\n",
    "            qdrant = 'FP'\n",
    "        elif sample in tn:\n",
    "            qdrant = 'TN'\n",
    "        else:\n",
    "            qdrant = 'FN'\n",
    "        sample = list(sample.values())[0]\n",
    "        sample['qdrant'] = qdrant\n",
    "        final_samples.append(sample)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    selected_samples = list(np.random.choice(final_samples,size=num_samples))\n",
    "    embed_points = [\n",
    "        \"\\n\".join(list(data['input'].values()))\n",
    "        for data in selected_samples\n",
    "    ]\n",
    "    vectors = embeddings.embed_documents(embed_points)\n",
    "    return selected_samples,np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "741c14be-ce86-42c6-90d3-0ded017aa503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_save(data,path):\n",
    "    with open(path,\"w\") as file:\n",
    "        json.dump(data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74a7b607-38f5-4ed9-9db7-a7b20a49f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_samples, vectors = embed(num_samples=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4d770030-166e-4166-afa0-bd134d3128ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TN', 9), ('TP', 6), ('FP', 4), ('FN', 1)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([sample['qdrant'] for sample in vector_samples]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12dba73c-7a7b-4239-af49-d4fdbd2ace6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('indices/selected_20_indices_input_resp_ref.npy', vectors)\n",
    "json_save(vector_samples,'indices/selected_20_indices_input_resp_ref.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d75ad9c-2ae3-4f61-b0cd-72cb52a597be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open(\"indices/selected_20_indices.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94a76b4c-8f1c-4c04-aeb5-5d4341e03b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(data):\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        if item['qdrant'] in ['TP','FN']:\n",
    "            item['target'] = 1\n",
    "        elif item['qdrant'] in ['TN','FP']:\n",
    "            item['target'] = 0\n",
    "        new_data.append(item)\n",
    "    return new_data\n",
    "\n",
    "data = add_target(data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8615f8-e65f-4df6-b098-30ba60966f26",
   "metadata": {},
   "source": [
    "### Test retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4129eba6-bcae-4cf2-99e6-8267adec35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics._aspect_critic import AspectCriticWithReference\n",
    "critic = AspectCriticWithReference(name=\"answer correctness\",\n",
    "                      definition=\"Given the user_input, reference and response. Is the response correct compared with the reference\",)\n",
    "\n",
    "th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4f5e92d-1f25-46e3-aa52-35118e82e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.embedding_model = langchain_embeddings\n",
    "critic.llm = llm_4o_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "300a4481-2f6a-437a-95f0-4cc2ade402a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample\n",
    "dataset = Dataset.from_json(\"datasets/dataset_v4.json\")\n",
    "eval_sample = SingleTurnSample(**dataset[0])\n",
    "eval_sample.user_input = \"What were the causes and consequences of the Chernobyl Disaster?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f35be5-fed2-47a1-acd7-d81969b3d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await critic.single_turn_ascore(eval_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc3491-a4ce-4183-9bc5-a3bd175bc7cc",
   "metadata": {},
   "source": [
    "## prompt optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff3159d3-6c3c-4ef3-b573-b4180ce2b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling import SingleExample,PromptFromCorrectExample\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4dcac3ae-cd13-4b88-a952-7a42d2543d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas.metrics._aspect_critic import AspectCriticInputWithReference, AspectCriticOutputWithReference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9a855c85-9c36-45e6-8c70-cfa87ab69301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dict(dic):\n",
    "    string = \"\"\n",
    "    for key, val in dic.items():\n",
    "        string += f\"\\n{key}:\\n\\t{val}\\n\"\n",
    "    return string\n",
    "\n",
    "async def reverse_engineer_instruction_from_correct_examples(data,llm,num_instructions=5, num_samples=10,seed=42):\n",
    "    \"\"\"\n",
    "    reverse engineer prompts using correct example\n",
    "    \"\"\"\n",
    "    generated_prompts = []\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    correct_examples = [sample for sample in data if sample['qdrant'] in ['TN','TP']]\n",
    "\n",
    "    prompt = PromptFromCorrectExample()\n",
    "    for i in tqdm(range(num_instructions)):\n",
    "        input_list, output_list = [],[]\n",
    "        \n",
    "        examples  = np.random.choice(correct_examples,size=3)\n",
    "        curr_examples = []\n",
    "        for example in examples:\n",
    "            _ = example['input'].pop('criteria') if 'criteria' in example['input'] else None\n",
    "            curr_examples.append((format_dict(example[\"input\"]),example['output'][0]))\n",
    "        prompt_input = SingleExample(examples=curr_examples)\n",
    "        response = await prompt.generate(data=prompt_input,llm=llm)\n",
    "        generated_prompts.append(response.instruction)\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7fbc886b-391a-4f4f-a811-de85e8e06e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 5/5 [00:12<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "candidate_prompts=await reverse_engineer_instruction_from_correct_examples(data,llm_4o_mini,num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "10d74471-2aea-4a9b-8e87-bd5d89c0af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def stratified_sample_with_fallback(dataset: Dataset, batch_size: int, target_column: str = 'target') -> Dataset:\n",
    "    \"\"\"\n",
    "    Function to sample a batch of data with as equal distribution of target values as possible.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (Dataset): The Hugging Face dataset to sample from.\n",
    "        batch_size (int): The number of samples in the output batch.\n",
    "        target_column (str): The column name containing the target values.\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: A batch of data with as close to equal distribution of target values as possible.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=42)\n",
    "    # Get all unique target values and count samples per target\n",
    "    target_values = dataset.unique(target_column)\n",
    "    target_counts = {target: 0 for target in target_values}\n",
    "    for example in dataset:\n",
    "        target_counts[example[target_column]] += 1\n",
    "    \n",
    "    # Calculate initial number of samples per class\n",
    "    num_classes = len(target_values)\n",
    "    samples_per_class = batch_size // num_classes\n",
    "    remaining_slots = batch_size\n",
    "\n",
    "    # Track sampled indices\n",
    "    sampled_indices = []\n",
    "    \n",
    "    # Sampling loop with fallback for insufficient samples\n",
    "    for target in target_values:\n",
    "        available_indices = [idx for idx, example in enumerate(dataset) if example[target_column] == target]\n",
    "        num_to_sample = min(samples_per_class, len(available_indices))\n",
    "        \n",
    "        # Sample available indices\n",
    "        sampled_indices.extend(random.sample(available_indices, num_to_sample))\n",
    "        remaining_slots -= num_to_sample\n",
    "    \n",
    "    # Handle remaining slots by resampling from classes with available samples\n",
    "    while remaining_slots > 0:\n",
    "        for target in target_values:\n",
    "            if remaining_slots <= 0:\n",
    "                break\n",
    "            available_indices = [idx for idx, example in enumerate(dataset) if example[target_column] == target]\n",
    "            if available_indices:\n",
    "                # Sample one additional index from available samples\n",
    "                sampled_indices.append(random.choice(available_indices))\n",
    "                remaining_slots -= 1\n",
    "\n",
    "    # Shuffle sampled indices to ensure randomness\n",
    "    random.shuffle(sampled_indices)\n",
    "    return dataset.select(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a7db7c92-0535-43c0-b0a3-0199b1a30fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prompts(dataset,candidate_prompts,batch_size,metric_kwargs={}):\n",
    "    result_scores = []\n",
    "    for prompt in candidate_prompts:\n",
    "        critic = AspectCriticWithReference(name=\"answer correctness\",\n",
    "                          definition=prompt,**metric_kwargs)\n",
    "        dataset = stratified_sample_with_fallback(dataset,batch_size,target_column='qdrant')\n",
    "        y_true = dataset['target']\n",
    "        eval_dataset = [data['input'] for data in dataset]\n",
    "        eval_dataset = EvaluationDataset.from_list(eval_dataset)\n",
    "        result = evaluate(dataset=eval_dataset,metrics=[critic])\n",
    "        traces = result.traces\n",
    "        df = result.to_pandas()\n",
    "        y_pred = df[\"answer correctness\"].values.tolist()\n",
    "        incorrect_indices = [i for i in range(len(y_pred)) if y_pred[i]!=y_true[i]]\n",
    "        feedback_samples = []\n",
    "        for idx in incorrect_indices:\n",
    "            if dataset[idx]['qdrant'] in ['TP','TN']:\n",
    "                dic ={\n",
    "                    \"input\":dataset[idx][\"input\"],\n",
    "                    \"incorrect_output\":serialize_for_json(traces[idx]['answer correctness']),\n",
    "                    \"expected_output\":dataset[idx][\"output\"][0]\n",
    "                }\n",
    "                dic['incorrect_output'] = dic['incorrect_output']['0_single_turn_aspect_critic_prompt_with_reference']['output'][0]\n",
    "                feedback_samples.append(dic)\n",
    "        fscore = f1_score(y_true,y_pred)\n",
    "        result_scores.append({\"prompt\":prompt,\"score\":fscore,\"feedback\":feedback_samples})\n",
    "    return result_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bebc6c07-60ea-432d-836e-e2ab88d2b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████| 10/10 [00:03<00:00,  2.90it/s]\n",
      "Flattening the indices: 100%|█| 10/10 [00:00<00:00, 2198.8\n",
      "Evaluating: 100%|█████████| 10/10 [00:02<00:00,  4.09it/s]\n",
      "Flattening the indices: 100%|█| 10/10 [00:00<00:00, 2354.5\n",
      "Evaluating: 100%|█████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Flattening the indices: 100%|█| 10/10 [00:00<00:00, 2256.7\n",
      "Evaluating: 100%|█████████| 10/10 [00:04<00:00,  2.42it/s]\n",
      "Flattening the indices: 100%|█| 10/10 [00:00<00:00, 1755.3\n",
      "Evaluating: 100%|█████████| 10/10 [00:03<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "training_dataset = Dataset.from_list(data)\n",
    "metric_kwargs = {\n",
    "    \"llm\":llm_4o_mini,\n",
    "    \"embedding_model\":embeddings,\n",
    "    \"dynamic_retrevial\": False\n",
    "}\n",
    "prompt_scores = score_prompts(training_dataset,candidate_prompts[:],10,metric_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f1be8683-c53f-4fa3-876f-5e0a4a030f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling import FeedbackExample, FeedbackMutationInput, FeedbackMutationPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b3efa67d-6324-4848-800d-5d2b4253b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_feedback_for_prompt(prompt,llm):\n",
    "    mutation_prompt = FeedbackMutationPrompt()\n",
    "    original_prompt = prompt['prompt']\n",
    "    examples = []\n",
    "    for item in prompt['feedback']:\n",
    "        input_ = item['input']\n",
    "        if 'criteria' in input_:\n",
    "            _ = input_.pop(\"criteria\")\n",
    "        input_ = format_dict(input_)\n",
    "        prompt_input = FeedbackExample(input=input_,output=item[\"incorrect_output\"],expected_output=item[\"expected_output\"])\n",
    "        examples.append(prompt_input)\n",
    "    prompt_input = FeedbackMutationInput(prompt=original_prompt,examples=examples)\n",
    "    response = await mutation_prompt.generate(data=prompt_input,llm=llm)\n",
    "    return response.feedbacks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e52598fc-457a-4a9e-904c-d2f5a67cd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks = await get_feedback_for_prompt(prompt_scores[0],llm_4o_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9ae54261-9d7e-4ef3-8d6e-9ea30ea76e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clarify the criteria for assigning a verdict of 1 or 0, ensuring that the prompt explicitly states that a verdict of 1 should be given for responses that align closely with the reference, even if they contain some additional context or elaboration.',\n",
       " 'Emphasize the importance of including all critical details from the reference in the response, such as specific phrases or terms that are essential for accuracy, to avoid missing key information that could affect the verdict.',\n",
       " 'Encourage a more concise evaluation of responses, suggesting that while additional context can be informative, it should not detract from the core factual accuracy and relevance to the reference.']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "058ff468-fc62-4d52-b3bc-ce731e20e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling import FeedbackMutationPromptGeneration,FeedbackMutationPromptInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "64314106-36a7-444f-9535-baac7dc8d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_prompt_from_feedback(prompt,feedbacks,llm):\n",
    "    improvement_prompt = FeedbackMutationPromptGeneration()\n",
    "    prompt_input = FeedbackMutationPromptInput(prompt=prompt,feedbacks=feedbacks)\n",
    "    response = await improvement_prompt.generate(data=prompt_input,llm=llm)\n",
    "    return response.instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "aa19d831-404a-4055-9968-3d6f52710744",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = await generate_prompt_from_feedback(prompt_scores[0]['prompt'],feedbacks,llm_4o_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "65f69fd2-1e1a-4344-95e1-54cee8af815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please evaluate the provided responses based on their accuracy and relevance to the user inputs and references. Assign a verdict of 1 for responses that align closely with the reference, even if they contain some additional context or elaboration, and 0 for incorrect responses that contain factual inaccuracies or significant deviations from the reference. Ensure that all critical details from the reference, including specific phrases or terms essential for accuracy, are included in the response to avoid missing key information that could affect the verdict. Provide a brief reason for your verdict, highlighting both strengths and weaknesses in the responses, and ensure that the evaluation remains concise, focusing on core factual accuracy and relevance to the reference. For guidance, examples of significant deviations include discrepancies in key dates or misinterpretations of critical facts.'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_promptz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8f21445f-c1b1-4ba0-aed3-3fd44638bd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████| 10/10 [00:04<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_scores = score_prompts(training_dataset,[new_prompt],10,metric_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "60b594a9-c07b-4988-a1bc-ae1ad2059eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'Please evaluate the provided responses based on their accuracy and relevance to the user inputs and references. Assign a verdict of 1 for responses that align closely with the reference, even if they contain some additional context or elaboration, and 0 for incorrect responses that contain factual inaccuracies or significant deviations from the reference. Ensure that all critical details from the reference, including specific phrases or terms essential for accuracy, are included in the response to avoid missing key information that could affect the verdict. Provide a brief reason for your verdict, highlighting both strengths and weaknesses in the responses, and ensure that the evaluation remains concise, focusing on core factual accuracy and relevance to the reference. For guidance, examples of significant deviations include discrepancies in key dates or misinterpretations of critical facts.',\n",
       "  'score': 0.7272727272727273,\n",
       "  'feedback': []}]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "023c92ac-d96d-484c-aee6-4ccbfe111edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"indices/selected_20_indices_input_resp_ref.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "760021ce-c151-4c21-bd14-8aa915f8daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [item['input'].pop('criteria') for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "825c1103-8897-4d69-8515-bfcac21bc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save(data,'indices/selected_20_indices_input_resp_ref.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681af3f-a688-4e02-a27f-98348ba465ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
