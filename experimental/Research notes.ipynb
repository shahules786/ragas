{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4fa2fe",
   "metadata": {},
   "source": [
    "## Log of changes\n",
    "- changes sent tokenizers with pysbd\n",
    "    - think about change as change will affect processing in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db0b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import wikipediaapi\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "openai.api_key = json.load(open(\"/Users/shahules/openai-key.json\"))['ikka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9266b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baaee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You're a bot that answers any given question. If you dont know the exact answer make up one.\"},\n",
    "{\"role\":\"user\", \"content\":\"What were the temperatures and snowfall amounts during the cold snap in Afghanistan in January 2023, and how many people and livestock were affected?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32da92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641c1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Black hole\")\n",
    "\n",
    "\n",
    "def get_page_section(page,chars=8000):\n",
    "    all_text = \"\"\n",
    "    p_wiki = wiki_wiki.page(page)    \n",
    "    return p_wiki.text[:chars]\n",
    "\n",
    "\n",
    "def get_cosine(page, backlinks):\n",
    "    backlinks_vec = model.encode(backlinks)\n",
    "    page_vec = model.encode([page]).reshape(1,-1)\n",
    "    norm = np.linalg.norm(backlinks_vec,axis=1)*np.linalg.norm(page_vec,axis=1)\n",
    "    cosine_sim = np.dot(backlinks_vec,page_vec.T).reshape(-1,)/norm\n",
    "    return cosine_sim\n",
    "\n",
    "def get_backlink_titles(page):\n",
    "    p_wiki = wiki_wiki.page(page) \n",
    "    backlinks =  [i.title() for i in p_wiki.backlinks if \":\" not in i.title()][:100]\n",
    "    if len(backlinks)>1:\n",
    "        c = get_cosine(page, backlinks)\n",
    "        top_indices = c.argsort()[::-1][:10]\n",
    "        return [backlinks[i] for i in top_indices if backlinks[i]!=page]\n",
    "    return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee43d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question_gen = \"\"\"\n",
    "Given a wikipedia text generate a question that can be fully answered from the text of medium difficulty with long form answer seperated by \\n\\n from the given text.\n",
    "\n",
    "Wikipedia Text:\n",
    "The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel, whose company designed and built the structure. Erected in 1889 as the entrance arch to the 1889 World's Fair, it has become a global cultural icon of France and one of the most recognizable structures in the world. The Eiffel Tower is the tallest structure in Paris and the most-visited paid monument in the world; millions of people ascend it every year. The tower stands at a height of 324 meters (1,063 feet) and was the tallest man-made structure in the world until the completion of the Chrysler Building in New York City in 1930.\n",
    "The tower has three levels for visitors, with restaurants on the first and second levels. The third level observatory's upper platform is at 276 meters (906 feet) above the ground, the highest accessible to the public in the European Union. The tower has been featured in numerous films and TV shows, and its lighting is often modified to mark special events or holidays. Despite initial criticism from some of France's leading artists and intellectuals, the Eiffel Tower has become a global cultural icon of France and one of the most recognizable structures in the world.\n",
    "Question: What is the height of the Eiffel Tower, and how does it compare to the height of the Chrysler Building in New York City?\n",
    "\\n\\nAnswer:The height of the Eiffel Tower is 324 meters (1,063 feet). It was the tallest man-made structure in the world until the completion of the Chrysler Building in New York City in 1930. The Chrysler Building surpassed the Eiffel Tower's height and became the new tallest structure, reaching a height of 319 meters (1,046 feet) including its spire. However, it's important to note that the Eiffel Tower remains the tallest structure in Paris and still holds its title as the most-visited paid monument in the world. Despite losing its status as the tallest man-made structure globally, the Eiffel Tower's cultural significance and iconic status endure, attracting millions of visitors each year and leaving an indelible mark on the city of Paris and the world at large.\n",
    "Wikipedia Text:\\n{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850b2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_dataset(pages):\n",
    "    data = []\n",
    "    for page in tqdm(pages):\n",
    "        try:\n",
    "            text = get_page_section(page)\n",
    "            message = [{\"role\": \"user\", \"content\":Question_gen.format(text)}]\n",
    "            output = llm(message)\n",
    "            question, answer_grounded = output['choices'][0]['message']['content'].split(\"\\n\\n\")\n",
    "            message = [{\"role\": \"user\", \"content\":question}]\n",
    "            answer = llm(message)['choices'][0]['message']['content']\n",
    "            data.append({\n",
    "                \"question\":question,\n",
    "                \"grounded_answer\":answer_grounded,\n",
    "                \"answer\":answer,\n",
    "                \"context\":text,\n",
    "                \"source\":page,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6378ff7",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/2023#January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb2ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/shahules/belar/experimental/pages.txt\", \"r\") as file:\n",
    "    pages = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "608f6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = llm(Question_gen.format(c.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aebbacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, answer = output['choices'][0]['message']['content'].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711eca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pages.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64c93114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\"Volcanism on Venus\",\"Pandemic prevention\",\"Jupiter Icy Moons Explorer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ad99e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 3/3 [00:54<00:00, 18.14s/it]\n"
     ]
    }
   ],
   "source": [
    "data_sample = generate_dataset(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf5c68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"/Users/shahules/belar/experimental/ragas_wiki_eval.json\"))\n",
    "data.extend(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49a187f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91b678ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragas-eval-data.json\",'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "919d4860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorn-khiri Sriprachuap\n",
      "Methane synthesis\n",
      "1993–94 BCFC season\n",
      "Hagawi\n",
      "Populus mexicana\n",
      "Gracie Lawrence\n",
      "Gracie Bea Lawrence\n",
      "Laura Cornelius\n",
      "A. D. Macklin\n",
      "Polyosma hirsuta\n",
      "Maltzanella\n",
      "Molina Seca\n",
      "The Life of the Party (play)\n",
      "Ramnagar Assembly constituency (disambiguation)\n",
      "Blue Angel (Park Ji-yoon album\n",
      "A.D. Macklin\n",
      "Blanca Wiethüchter López\n",
      "Frances Hughes\n",
      "Gubad Ibadoghlu\n",
      "2023 BWF World Championships – Men's doubles\n",
      "Jesús Urzagasti Aguilera\n",
      "1925 Reform Party (New Zealand) leadership election\n",
      "Ana Pinho Rodrigues\n",
      "Olympian 6\n",
      "80th Venice Film Festival\n",
      "2023 Venice Film Festival\n",
      "2023 Venice International Film Festival\n",
      "We Find the Bunyip\n",
      "C-ya-laterrrr\n",
      "Evan asano\n",
      "13th Politburo Standing Committee of the Chinese Communist Party\n",
      "Chase Blasi\n",
      "Hateful speech\n",
      "Template:WIR-280\n",
      "Archaeology in Africa\n",
      "Home (Secret Invasion)\n",
      "Anil Thadani\n",
      "Love Me a Little\n",
      "Wayne Stevens (disambiguation)\n",
      "Wayne Osborne (disambiguation)\n",
      "Wayne Hughes (disambiguation)\n",
      "Wayne Hammond (disambiguation)\n",
      "Thai Empire (disambiguation)\n",
      "Oxygen concentration (disambiguation)\n",
      "Danila Kozlov (disambiguation)\n",
      "Taivoan (disambiguation)\n",
      "Salvador Bernal (disambiguation)\n",
      "Gazo (disambiguation)\n",
      "T52 (disambiguation)\n",
      "Sergheevca (disambiguation)\n",
      "Wayne Coffey (disambiguation)\n",
      "Assistant commandant of the Marine Corps\n",
      "Lauren Anne Dickason\n",
      "Lauren Dickason\n",
      "Wikipedia talk:ANVDL\n",
      "Kira Kira, Papua New Guinea\n",
      "Pandemic of 1889-1893\n",
      "BU College of Fine Arts\n",
      "Stephen W. Gard\n",
      "HueUni\n",
      "Oscar Cerruto Collier\n",
      "Matilde Casazola Mendoza\n",
      "In a Room7 F760\n",
      "Zin2 Test5\n",
      "Kirakira (disambiguation)\n",
      "SunTec Group\n",
      "Oxygen hood\n",
      "Francisca Yu-Tsz Chang\n",
      "Athra Alliance\n",
      "Christine Tappolet\n",
      "New Media Writing Prize\n",
      "Mercedes Belzu Gorriti\n",
      "Yolanda Bedregal de Cónitzer\n",
      "Lindaura Anzoátegui de Campero\n",
      "Alcides Arguedas Díaz\n",
      "Supplemental weaving\n",
      "Albanian raid Iranian dissidents\n",
      "Brazil–Libya relations\n",
      "Siegfried Wouthuysen\n",
      "Blackbox Life Recorder 21f / In a Room7 F760\n",
      "West German cinema\n",
      "Leslie Foldy\n",
      "Skorn\n",
      "COZ (disambiguation)\n",
      "CEM (disambiguation)\n",
      "CEP (disambiguation)\n",
      "CAM (disambiguation)\n",
      "Non resistant\n",
      "Nonresistant protest\n",
      "City of Berkeley Landmark\n",
      "Functional textiles\n",
      "Ubilava\n",
      "Category talk:Quebec Cinema Awards navigational boxes\n",
      "Allison Rose\n",
      "María de las Mercedes of Orléans\n",
      "Spinning Globe (song)\n",
      "Spinning Globe\n",
      "Ina Mester\n",
      "Template:Editnotices/Page/Mick Jagger\n",
      "List of Santana band members\n",
      "Sound effect (musical instruments)\n",
      "Sound effect (plays and movies)\n",
      "Mahan (1996 film)\n",
      "2023 Spanish Election\n",
      "Table tennis at the 2023 European Games – Women's singles\n",
      "Spanish election, 2023\n",
      "Spanish election 2023\n",
      "Wenden-Thune-Harxbüttel\n",
      "Thune (Braunschweig)\n",
      "Polaris Building\n",
      "Template:Old good article table entry/doc\n",
      "Sound Affects (album)\n",
      "Pikciute\n",
      "Pikčiutė\n",
      "BIM (disambiguation)\n",
      "Universal Music Russia\n",
      "BIQ (disambiguation)\n",
      "Robin Gollan\n",
      "Murder of Nurhidayati Wartono Surata\n",
      "Golden Dragon Hotel murder\n",
      "Liam Horne\n",
      "Nurhidayati Wartono Surata\n",
      "File talk:Echo & the Bunnymen - The Stars, the Oceans & the Moon.jpg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    get_recent_changes.py\n",
    "\n",
    "    MediaWiki API Demos\n",
    "    Demo of `RecentChanges` module: Get the three most recent changes with\n",
    "    sizes and flags\n",
    "\n",
    "    MIT License\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"format\": \"json\",\n",
    "    \"rcprop\": \"title|timestamp|tags\",\n",
    "    \"list\": \"recentchanges\",\n",
    "    \"action\": \"query\",\n",
    "    \"rclimit\": \"1000\",\n",
    "    \"rctype\":\"new\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "RECENTCHANGES = DATA['query']['recentchanges']\n",
    "remove_tags = [\"User:\",\"Category:\",\"Draft\",\"User talk:\",\"Talk:\",\"Wikipedia:\",\"Template talk:\"]\n",
    "\n",
    "for rc in RECENTCHANGES:\n",
    "    title = str(rc['title'])\n",
    "    timestamp = str(rc['timestamp'])\n",
    "    if not any([x in title for x in remove_tags]) and \"2021\" not in timestamp:\n",
    "        \n",
    "        print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "b074840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"ragas-data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6c20e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evading = [\"I'm sorry\", \"as an AI language model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9499102",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_regen = [item for item in data if any(x.lower() in item['answer'].lower() for x in evading)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ca6c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You're a bot that answers any given question. If you dont know the exact answer make up one.\"},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "47cf1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in topics_regen:\n",
    "    message = [{\"role\": \"system\", \"content\": \"You're a bot that answers any given question. If you dont know the exact answer make up one.\"},]\n",
    "    message.append({\"role\":\"user\",\"content\":item['question']})\n",
    "    output = llm(message)\n",
    "    item['answer'] = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a21cff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 50/50 [01:23<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(data):\n",
    "    titles = get_backlink_titles(item[\"source\"])\n",
    "    context = [get_page_section(item,chars=1000) for item in titles[:2]]\n",
    "    context.insert(0,item['context'])\n",
    "    item[\"context_retrieved\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af204254",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragas_wiki_evalv1.json\",'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67f1fe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/shahules/.cache/huggingface/datasets/json/default-d425d908fb97025a/0.0.0)\n",
      "Pushing dataset shards to the dataset hub:   0%|                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|█████████████████| 1/1 [00:00<00:00, 50.02ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████| 1/1 [00:05<00:00,  5.34s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n"
     ]
    }
   ],
   "source": [
    "Dataset.from_json(\"ragas-eval-data.json\").push_to_hub(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95745ce8",
   "metadata": {},
   "source": [
    "## Generate low relevancy answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307adfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a808a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_answer = \"\"\"\n",
    "Answer the given question partially.\n",
    "Question:{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ea7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--wiki-eval-f0df84e235efd078/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████| 1/1 [00:00<00:00, 89.46it/s]\n",
      "Extracting data files: 100%|████████| 1/1 [00:00<00:00, 389.52it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--wiki-eval-f0df84e235efd078/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 1/1 [00:00<00:00, 224.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1fa267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for item in dataset['train']:\n",
    "    question = item['question']\n",
    "    message = []\n",
    "    message.append({\"role\":\"user\",\"content\":irr_answer.format(question)})\n",
    "    while True:\n",
    "        try:\n",
    "            answer = llm(message)['choices'][0]['message']['content']\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        break\n",
    "    answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b28ee072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].add_column(\"partial_answer\",answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f19512c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%| | 0/1 [00:00<?, ?i\n",
      "Creating parquet from Arrow format: 100%|█| 1/1 [00:00<00:00, 79.01\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                    | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|████████████| 1/1 [00:06<00:00,  6.60s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|█| 1/1 [00:07<00:00\n",
      "Deleting unused files from dataset repository: 100%|█| 1/1 [00:00<0\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "34f604e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragas_wiki_evalv1.json\",'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c8b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--wiki-eval-80d4ef132547f2df/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 241.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601e61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select_columns([\"question\",\"source\",\"answer\",\"grounded_answer\",\"answer_relevancy_v1\",\"context_v1\",\"context_v2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ae0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"answer\":\"ungrounded_answer\",\"answer_relevancy_v1\":\"poor_answer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b36b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Pushing dataset shards to the dataset hub:   0%|                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|█████████████████| 1/1 [00:00<00:00, 97.16ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████| 1/1 [00:06<00:00,  6.32s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:07<00:00,  7.51s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "Downloading metadata: 100%|████████████████████████| 1.06k/1.06k [00:00<00:00, 3.07MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5fe5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def record_score(scores,name,filename='ragas_promptscoring'):\n",
    "    data = json.load(open(f\"{filename}.json\"))\n",
    "    data[name] = scores\n",
    "    with open(f\"{filename}.json\",'w') as file:\n",
    "        json.dump(data,file,indent=4)\n",
    "        \n",
    "def read_score(score_name):\n",
    "    data = json.load(open(\"ragas_abblation.json\"))\n",
    "    return data[score_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c141fe",
   "metadata": {},
   "source": [
    "## Ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9ecfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cef7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas.metrics.context_relevance import ContextRelevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "185afc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevancy = ContextRelevancy(strictness=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc427791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--WikiEval-3b60abf6f625ac40/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.36it/s]\n"
     ]
    }
   ],
   "source": [
    "wikieval = load_dataset(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dd99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval = wikieval['train'].rename_columns({\"grounded_answer\":\"answer\",\"context_v1\":\"contexts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186a4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval1 = wikieval.select_columns(['answer', 'question','contexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736e4e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [04:54<00:00, 73.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 4/4 [08:41<00:00, 130.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [02:01<00:00, 30.46s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset=wikieval1,metrics=[context_relevancy,faithfulness,answer_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ee8159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|█████████████████████| 1/1 [00:00<00:00, 10.41ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87063"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikieval1.select(range(0,20)).to_csv(\"/Users/shahules/Downloads/wikieval_sample20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5abe01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.6130, 'context_relevancy': 0.3521, 'faithfulness': 0.9865, 'answer_relevancy': 0.9613}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f2f901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Answer: The PSLV-C56 mission is scheduled to b...</td>\n",
       "      <td>Question: When is the scheduled launch date an...</td>\n",
       "      <td>[The PSLV-C56 is the 58th mission of Indian Sp...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer: The objective of the Uzbekistan-Afghan...</td>\n",
       "      <td>Question: What is the objective of the Uzbekis...</td>\n",
       "      <td>[The Uzbekistan–Afghanistan–Pakistan Railway P...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer: PharmaCann was founded in 2014 by Theo...</td>\n",
       "      <td>Question: When was PharmaCann founded and what...</td>\n",
       "      <td>[Found in 2014 by Theodore Scott, PharmaCann i...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answer: Christopher Nolan directed the film Op...</td>\n",
       "      <td>Question: Who directed the film Oppenheimer an...</td>\n",
       "      <td>[Oppenheimer is a 2023 biographical thriller f...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Answer: Theranostics, also known as theragnost...</td>\n",
       "      <td>Question: What is theranostics and how does it...</td>\n",
       "      <td>[Theranostics, also known as theragnostics, is...</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Answer: The human climate niche refers to the ...</td>\n",
       "      <td>Question: What is the human climate niche and ...</td>\n",
       "      <td>[The human climate niche is the ensemble of cl...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Answer: Dasypoda radchenkoi belongs to the gen...</td>\n",
       "      <td>Question: What is the taxonomy of Dasypoda rad...</td>\n",
       "      <td>[Dasypoda radchenkoi, also known as Radchenko'...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Answer: The main product of Fremantle Octopus ...</td>\n",
       "      <td>Question: What is the main product of Fremantl...</td>\n",
       "      <td>[Fremantle Octopus is an Australian octopus fi...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Answer: The Managing Director of FoodFutureCo ...</td>\n",
       "      <td>Question: Who is the Managing Director of Food...</td>\n",
       "      <td>[FoodFutureCo is a scale-up accelerator for pu...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Answer: The purpose of designing and building ...</td>\n",
       "      <td>Question: What was the purpose of designing an...</td>\n",
       "      <td>[The Fiat Ecobasic is a concept car designed b...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Answer: The Rainbow Plaque programme in the UK...</td>\n",
       "      <td>Question: What is the purpose of the Rainbow P...</td>\n",
       "      <td>[The Rainbow Plaque programme is a UK scheme i...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Answer: The Zubaydah Trail, also known as the ...</td>\n",
       "      <td>Question: What is the Zubaydah Trail and when ...</td>\n",
       "      <td>[The Zubaydah Trail (Al-Kufi pilgrimage route)...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Answer: The Chimnabai Clock Tower was complete...</td>\n",
       "      <td>Question: When was the Chimnabai Clock Tower c...</td>\n",
       "      <td>[The Chimnabai Clock Tower, also known as the ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Answer: Trolleybus Route 20 in Shanghai starte...</td>\n",
       "      <td>Question: When did Trolleybus Route 20 in Shan...</td>\n",
       "      <td>[Trolleybus Route 20 is a trolleybus route in ...</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Answer: The Inter Expo Center in Sofia, Bulgar...</td>\n",
       "      <td>Question: When did the Inter Expo Center in So...</td>\n",
       "      <td>[The Inter Expo Center (IEC) is a multi-purpos...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Answer: Pope Benedict XVI became the head of t...</td>\n",
       "      <td>Question: When did Pope Benedict XVI become th...</td>\n",
       "      <td>[Pope Benedict XVI (Latin: Benedictus PP. XVI;...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Answer: The crash of Yeti Airlines Flight 691 ...</td>\n",
       "      <td>Question: What caused the crash of Yeti Airlin...</td>\n",
       "      <td>[Yeti Airlines Flight 691 was a scheduled dome...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Answer: The Starship rocket has a height of 12...</td>\n",
       "      <td>Question: How does the height and thrust of th...</td>\n",
       "      <td>[Starship is a super heavy-lift launch vehicle...</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Answer: The Kyzylkum Desert is known for its d...</td>\n",
       "      <td>Question: What is the Kyzylkum Desert known fo...</td>\n",
       "      <td>[The Kyzylkum Desert (Uzbek: Qizilqum, Қизилқу...</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Answer: The 80th annual Venice International F...</td>\n",
       "      <td>Question: When will the 80th annual Venice Int...</td>\n",
       "      <td>[The 80th annual Venice International Film Fes...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Answer: Myosotis angustata is endemic to the S...</td>\n",
       "      <td>Question: Where is Myosotis angustata endemic ...</td>\n",
       "      <td>[Myosotis angustata is a species of flowering ...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Answer: Mount Brown is located in Liberty Coun...</td>\n",
       "      <td>Question: Where is Mount Brown located and wha...</td>\n",
       "      <td>[Mount Brown is a 6,958-foot-elevation (2,121-...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Answer: The type locality of the Laoshan tree ...</td>\n",
       "      <td>Question: Where is the type locality of the La...</td>\n",
       "      <td>[The Laoshan tree frog (Rhacophorus laoshan) i...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Answer: The Roanoke and Tar River Railroad was...</td>\n",
       "      <td>Question: When was the Roanoke and Tar River R...</td>\n",
       "      <td>[The Roanoke and Tar River Railroad was a rail...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Answer: Moud Goba is one of the founding membe...</td>\n",
       "      <td>Question: What organizations has Moud Goba bee...</td>\n",
       "      <td>[Moud Goba is a Zimbabwean LGBTIQ+ human right...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Answer: The purpose of the Modernizing Opioid ...</td>\n",
       "      <td>Question: What is the purpose of the Modernizi...</td>\n",
       "      <td>[The Modernizing Opioid Treatment Access Act i...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Answer: Gaucho Americano had its world premier...</td>\n",
       "      <td>Question: When and where did Gaucho Americano ...</td>\n",
       "      <td>[Gaucho Americano (lit. 'American Gaucho') is ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Answer: The type locality of the Blakistonia p...</td>\n",
       "      <td>Question: Where is the type locality of the Bl...</td>\n",
       "      <td>[Blakistonia plata is a species of mygalomorph...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Answer: In the 2022–23 season, the Turkish Wom...</td>\n",
       "      <td>Question: How many teams participate in the Tu...</td>\n",
       "      <td>[The Turkish Women's Football Super League (Tu...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Answer: The Tomb of Alexander Stewart, also kn...</td>\n",
       "      <td>Question: Who is buried in the Tomb of Alexand...</td>\n",
       "      <td>[The Tomb of Alexander Stewart (or Tomb of the...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Answer: The 5th Separate Guards Tatsin Red Ban...</td>\n",
       "      <td>Question: When was the 5th Separate Guards Tat...</td>\n",
       "      <td>[The 5th Separate Guards Tatsin Red Banner Ord...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Answer: The Siege of Mariupol began on 24 Febr...</td>\n",
       "      <td>Question: How long did the Siege of Mariupol l...</td>\n",
       "      <td>[The Siege of Mariupol began on 24 February 20...</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Answer: A large number of countries, including...</td>\n",
       "      <td>Question: Which countries and international or...</td>\n",
       "      <td>[International sanctions have been imposed aga...</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Answer: The Sri Lankan economic crisis was cau...</td>\n",
       "      <td>Question: What factors contributed to the Sri ...</td>\n",
       "      <td>[The Sri Lankan economic crisis is an ongoing ...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Answer: The 2022 Hormozgan earthquakes were a ...</td>\n",
       "      <td>Question: How many people were killed and inju...</td>\n",
       "      <td>[The 2022 Hormozgan earthquakes were a pair of...</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Answer: Seven people were killed and 48 others...</td>\n",
       "      <td>Question: How many people were killed and inju...</td>\n",
       "      <td>[On July 4, 2022, a mass shooting occurred dur...</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Answer: Uber has been involved in a number of ...</td>\n",
       "      <td>Question: What are some of the controversies s...</td>\n",
       "      <td>[Uber Technologies, Inc.  or Uber has been the...</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Answer: Initial estimates were that up to four...</td>\n",
       "      <td>Question: What was the estimated timeline for ...</td>\n",
       "      <td>[On December 3, 2022, a shooting attack was ca...</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Answer: The ethnic violence in Manipur in 2023...</td>\n",
       "      <td>Question: What caused the ethnic violence in M...</td>\n",
       "      <td>[On 3 May 2023, ethnic violence erupted in Ind...</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Answer: The Chinese balloon that was spotted i...</td>\n",
       "      <td>Question: What was the size and payload of the...</td>\n",
       "      <td>[From January 28 to February 4, 2023, a high-a...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Answer: During the cold snap in Afghanistan in...</td>\n",
       "      <td>Question: What were the temperatures and snowf...</td>\n",
       "      <td>[A cold snap began in Afghanistan on January 1...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Answer: GPT-4 was released on March 14, 2023. ...</td>\n",
       "      <td>Question: When was GPT-4 released and what are...</td>\n",
       "      <td>[Generative Pre-trained Transformer 4 (GPT-4) ...</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Answer: As of September 2022, the Myanmar civi...</td>\n",
       "      <td>Question: What is the current status of the My...</td>\n",
       "      <td>[The Myanmar civil war (Burmese: ၂၀၂၁-၂၀၂၃ မြန...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.941080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Answer: The Miss Grand Dominican Republic 2023...</td>\n",
       "      <td>Question: When and where will the Miss Grand D...</td>\n",
       "      <td>[Miss Grand Dominican Republic 2023 will be th...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Answer: The rebellion staged by the Wagner Gro...</td>\n",
       "      <td>Question: What was the cause of the rebellion ...</td>\n",
       "      <td>[On 23 June 2023, the Wagner Group, a Russian ...</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Answer: The gas supply outage in Sheffield, En...</td>\n",
       "      <td>Question: What caused the gas supply outage in...</td>\n",
       "      <td>[The city of Sheffield, England was impacted b...</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Answer: The civil unrest and protests in Iran ...</td>\n",
       "      <td>Question: What sparked the civil unrest and pr...</td>\n",
       "      <td>[Civil unrest and protests against the governm...</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Answer: On Venus, there are shield volcanoes, ...</td>\n",
       "      <td>Question: What types of volcanoes are found on...</td>\n",
       "      <td>[The surface of Venus is dominated by volcanic...</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.966426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Answer: Some measures for pandemic prevention ...</td>\n",
       "      <td>Question: What are some measures for pandemic ...</td>\n",
       "      <td>[Pandemic prevention is the organization and m...</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Answer: The main science objectives of the JUI...</td>\n",
       "      <td>Question: What are the main science objectives...</td>\n",
       "      <td>[The JUpiter ICy moons Explorer (Juice, former...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.953931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer  ... answer_relevancy\n",
       "0   Answer: The PSLV-C56 mission is scheduled to b...  ...         0.942336\n",
       "1   Answer: The objective of the Uzbekistan-Afghan...  ...         0.949762\n",
       "2   Answer: PharmaCann was founded in 2014 by Theo...  ...         0.963253\n",
       "3   Answer: Christopher Nolan directed the film Op...  ...         0.984808\n",
       "4   Answer: Theranostics, also known as theragnost...  ...         0.981180\n",
       "5   Answer: The human climate niche refers to the ...  ...         0.981146\n",
       "6   Answer: Dasypoda radchenkoi belongs to the gen...  ...         0.969072\n",
       "7   Answer: The main product of Fremantle Octopus ...  ...         0.971285\n",
       "8   Answer: The Managing Director of FoodFutureCo ...  ...         0.977580\n",
       "9   Answer: The purpose of designing and building ...  ...         0.981272\n",
       "10  Answer: The Rainbow Plaque programme in the UK...  ...         0.961344\n",
       "11  Answer: The Zubaydah Trail, also known as the ...  ...         0.921753\n",
       "12  Answer: The Chimnabai Clock Tower was complete...  ...         0.978963\n",
       "13  Answer: Trolleybus Route 20 in Shanghai starte...  ...         0.986743\n",
       "14  Answer: The Inter Expo Center in Sofia, Bulgar...  ...         0.965279\n",
       "15  Answer: Pope Benedict XVI became the head of t...  ...         0.984271\n",
       "16  Answer: The crash of Yeti Airlines Flight 691 ...  ...         0.974167\n",
       "17  Answer: The Starship rocket has a height of 12...  ...         0.931076\n",
       "18  Answer: The Kyzylkum Desert is known for its d...  ...         0.970542\n",
       "19  Answer: The 80th annual Venice International F...  ...         0.961350\n",
       "20  Answer: Myosotis angustata is endemic to the S...  ...         0.956137\n",
       "21  Answer: Mount Brown is located in Liberty Coun...  ...         0.984749\n",
       "22  Answer: The type locality of the Laoshan tree ...  ...         0.951691\n",
       "23  Answer: The Roanoke and Tar River Railroad was...  ...         0.961500\n",
       "24  Answer: Moud Goba is one of the founding membe...  ...         0.936848\n",
       "25  Answer: The purpose of the Modernizing Opioid ...  ...         0.972466\n",
       "26  Answer: Gaucho Americano had its world premier...  ...         0.971873\n",
       "27  Answer: The type locality of the Blakistonia p...  ...         0.969761\n",
       "28  Answer: In the 2022–23 season, the Turkish Wom...  ...         0.945319\n",
       "29  Answer: The Tomb of Alexander Stewart, also kn...  ...         0.971063\n",
       "30  Answer: The 5th Separate Guards Tatsin Red Ban...  ...         0.990852\n",
       "31  Answer: The Siege of Mariupol began on 24 Febr...  ...         0.941023\n",
       "32  Answer: A large number of countries, including...  ...         0.921415\n",
       "33  Answer: The Sri Lankan economic crisis was cau...  ...         0.979397\n",
       "34  Answer: The 2022 Hormozgan earthquakes were a ...  ...         0.917789\n",
       "35  Answer: Seven people were killed and 48 others...  ...         0.915272\n",
       "36  Answer: Uber has been involved in a number of ...  ...         0.971675\n",
       "37  Answer: Initial estimates were that up to four...  ...         0.902676\n",
       "38  Answer: The ethnic violence in Manipur in 2023...  ...         0.975990\n",
       "39  Answer: The Chinese balloon that was spotted i...  ...         0.963874\n",
       "40  Answer: During the cold snap in Afghanistan in...  ...         0.937017\n",
       "41  Answer: GPT-4 was released on March 14, 2023. ...  ...         0.979541\n",
       "42  Answer: As of September 2022, the Myanmar civi...  ...         0.941080\n",
       "43  Answer: The Miss Grand Dominican Republic 2023...  ...         0.966655\n",
       "44  Answer: The rebellion staged by the Wagner Gro...  ...         0.985703\n",
       "45  Answer: The gas supply outage in Sheffield, En...  ...         0.984112\n",
       "46  Answer: The civil unrest and protests in Iran ...  ...         0.935236\n",
       "47  Answer: On Venus, there are shield volcanoes, ...  ...         0.966426\n",
       "48  Answer: Some measures for pandemic prevention ...  ...         0.978360\n",
       "49  Answer: The main science objectives of the JUI...  ...         0.953931\n",
       "\n",
       "[50 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4c023",
   "metadata": {},
   "source": [
    "### Concat reannoated samples with all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f7bb2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--WikiEval-3b60abf6f625ac40/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 243.18it/s]\n",
      "Found cached dataset csv (/Users/shahules/.cache/huggingface/datasets/csv/default-7607020e99f8dfe0/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/shahules/.cache/huggingface/datasets/csv/default-6323ab1c8fd55177/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████| 1/1 [00:00<00:00, 1953.56it/s]\n",
      "Extracting data files: 100%|█████████████████████████████| 1/1 [00:00<00:00, 204.24it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/shahules/.cache/huggingface/datasets/csv/default-6323ab1c8fd55177/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "wikieval_og = load_dataset(\"explodinggradients/WikiEval\")\n",
    "wikieval_local = Dataset.from_csv(\"/Users/shahules/Downloads/wikieval_sample20 - wikieval_sample20.csv.csv\")\n",
    "wikieval_poor_answer = Dataset.from_csv(\"/Users/shahules/Downloads/wikieval_sample20 - poor_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a07605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_og = wikieval_og.rename_columns({\"grounded_answer\":\"answer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "016668cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval = wikieval_og['train'].select(range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd553008",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_local = wikieval_local.add_column('poor_answer', wikieval['poor_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b6ba878",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_local = wikieval_local.add_column('source', wikieval['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4dae604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/csv/default-7607020e99f8dfe0/0.0.0/cache-b666c3bbfd9a9a68.arrow\n",
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/csv/default-7607020e99f8dfe0/0.0.0/cache-51de8ac189b2e2d9.arrow\n"
     ]
    }
   ],
   "source": [
    "wikieval_local = wikieval_local.rename_columns({'contexts':'context_v1'})\n",
    "wikieval_local = wikieval_local.map(lambda x : {\"context_v1\":eval(x['context_v1'])})\n",
    "wikieval_local = wikieval_local.map(lambda x : {\"context_v2\":eval(x['context_v2'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2797e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43efb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_latest = concatenate_datasets([wikieval_local,wikieval_og['train'].select(range(20,50))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeef27e",
   "metadata": {},
   "source": [
    "- Update poor answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fca483c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_latest = wikieval_latest.remove_columns(['poor_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26237eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_latest = wikieval_latest.add_column('poor_answer',wikieval_poor_answer['poor_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa7efd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%|                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|█████████████████| 1/1 [00:00<00:00, 73.66ba/s]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:08<00:00,  8.11s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "wikieval_latest.push_to_hub(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab7ead",
   "metadata": {},
   "source": [
    "## reannotates sample\n",
    "Test assumptions\n",
    "1. Ragas scores ranked correctly\n",
    "2. GPT3.5 scores \n",
    "3. GPT3.5 rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3cdc498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# from ragas.metrics.context_relevance import sent_tokenize as ragas_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8e5d3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    \"question\":\"question\",\n",
    "#     \"contexts\":\"context_v2\",\n",
    "    \"answer\":\"poor_answer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d38f5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [02:17<00:00, 34.42s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset=wikieval_latest,\n",
    "                   metrics=[answer_relevancy],column_map=column_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "0411a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.3263, 'context_relevancy': 0.1971, 'answer_relevancy': 0.9478}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c2fbea1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9467473478661362,\n",
       " 0.9637259131522917,\n",
       " 0.9443463300116206,\n",
       " 0.9402534786633633,\n",
       " 0.9218596588819808,\n",
       " 0.9285405166500519,\n",
       " 0.9487620446850666,\n",
       " 0.9506784002084028,\n",
       " 0.941689354840754,\n",
       " 0.9588847769982651,\n",
       " 0.9318374062221277,\n",
       " 0.9516390195713166,\n",
       " 0.9483163635481464,\n",
       " 0.9603426519082335,\n",
       " 0.9266954346498673,\n",
       " 0.9575183534547804,\n",
       " 0.9801361309620589,\n",
       " 0.8827392620210031,\n",
       " 0.9381805505078088,\n",
       " 0.9464205724284924,\n",
       " 0.938312268188313,\n",
       " 0.9341558598962424,\n",
       " 0.9553525482187063,\n",
       " 0.9457937101525687,\n",
       " 0.9635201054966758,\n",
       " 0.986305022132583,\n",
       " 0.9472364344213897,\n",
       " 0.8825884294134566,\n",
       " 0.9796230393529424,\n",
       " 0.9265061660514919,\n",
       " 0.9693077277693484,\n",
       " 0.9546205206771519,\n",
       " 0.9561355788062716,\n",
       " 0.9668649935237948,\n",
       " 0.8428095549101032,\n",
       " 0.9439265136674045,\n",
       " 0.9473495611768238,\n",
       " 0.9525520918708642,\n",
       " 0.9578299924585187,\n",
       " 0.897650469966222,\n",
       " 0.9478177575220471,\n",
       " 0.9522007994869987,\n",
       " 0.9343958852735169,\n",
       " 0.9155273173058056,\n",
       " 0.9565743557995666,\n",
       " 0.966526310557822,\n",
       " 0.9555808438909955,\n",
       " 0.9580871766698351,\n",
       " 0.9606708520605401,\n",
       " 0.9463173411523016]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()['answer_relevancy'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c473e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_score(results.to_pandas()['faithfulness'].values.tolist(),\"ungrounded_faithfulness_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f9f2c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 0.75,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.33333333333333337,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.6666666666666667,\n",
       " 0.33333333333333337,\n",
       " 0.5,\n",
       " 0.33333333333333337,\n",
       " 0.6,\n",
       " 0.6666666666666667,\n",
       " 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()['faithfulness'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24176973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.6666666666666667,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()['faithfulness'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a3b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_matrix = np.vstack([read_score(\"gt_context_v1\"),read_score(\"context_v2_v1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb7b4031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_matrix[0,:] >= comp_matrix[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bee95b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3, 11]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(comp_matrix[0,:] < comp_matrix[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdabf8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ragas_tokenizer(wikieval1[1]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a51d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9c4e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(wikieval1[1]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33d329ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pysbd\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "len(seg.segment(wikieval1[1]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee453b",
   "metadata": {},
   "source": [
    "#### Annotate samples using gpt direct scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17f3e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"answer_relevancy\"\n",
    "definition = \"\"\"answer relevancy: refers to the degree to which a response directly addresses and is appropriate for a given question or context. Penalize incomplete or reduntant information in the answer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96257747",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "{definition}\n",
    "Given question and answer, assign a score for {metric_name} in the range 0-10.\n",
    "question:\\n{question}\n",
    "answer:\\n{answer}\n",
    "{metric_name} score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f1d841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = wikieval[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a47d61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = prompt.format(definition=definition,metric_name=metric_name,\n",
    "                             question=item['question'],\n",
    "                             answer=item['grounded_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1260be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7yHqQgOC2q8W4x876LorxX9Im4bP3 at 0x7f78d1192570> JSON: {\n",
       "  \"id\": \"chatcmpl-7yHqQgOC2q8W4x876LorxX9Im4bP3\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1694602630,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"8\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 115,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 116\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e146a9",
   "metadata": {},
   "source": [
    "### Annotate using gpt ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80597f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "{definition}\n",
    "Given question and two contexts , output rank of each context based on {metric_name}\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "output ranks:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c97b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20bb12c3",
   "metadata": {},
   "source": [
    "## Run in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfbaee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.metrics.llms import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0eaf9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =  ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "40943ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(dataset,prompt,llm,batch_size=15):\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0,len(dataset),batch_size):\n",
    "        prompts = []\n",
    "        ds = dataset.select(range(i,min(len(dataset),i+batch_size)))\n",
    "        answer1, answer2, question = ds[\"answer\"], ds[\"poor_answer\"], ds[\"question\"]\n",
    "        for q, a1, a2 in zip(question, answer1, answer2):\n",
    "#                 c_str: str = \"\\n\".join(c)\n",
    "                human_prompt = prompt.format(answer1=a2, answer2=a1, question=q)\n",
    "                prompts.append(ChatPromptTemplate.from_messages([human_prompt]))  \n",
    "        result = generate(prompts, llm)\n",
    "        result = [output[0].text for output in result.generations]\n",
    "        results.extend(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c2b39",
   "metadata": {},
   "source": [
    "## prompt scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2aa3cd",
   "metadata": {},
   "source": [
    "`<column_name>_<metric_name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "21e97941",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt = \"\"\"\n",
    "Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized\n",
    "Given an answer and context, assign a score for faithfulness in the range 0-10.\n",
    "\n",
    "answer:\\n{answer}\n",
    "context:\\n{context}\n",
    "faithfulness score:\n",
    "\"\"\"\n",
    "\n",
    "context_relevancy_prompt = \"\"\"\n",
    "Context Relevancy measures how relevant retrieved contexts are to the question. Ideally, the context should only contain information necessary to answer the question. The presence of redundant information in the context is penalized.\n",
    "Given an question and context, assign a score for context relevancy in the range 0-10.\n",
    "\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "context relevancy score:\n",
    "\"\"\"\n",
    "\n",
    "answer_relevancy_prompt = \"\"\"\n",
    "Answer Relevanc measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant information or incomplete answers given a question.\n",
    "Given an question and answer, assign a score for answer relevancy in the range 0-10.\n",
    "\n",
    "question:\\n{question}\n",
    "answer:\\n{answer}\n",
    "answer relevancy score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ebb9e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_batch(wikieval_latest.select(range(0,50)), prompt=faithfulness_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f08067ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [float(i) for i in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f3da2",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33889d8f",
   "metadata": {},
   "source": [
    "`We always provide the poor answer first and best answer last `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "235bd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt = \"\"\"\n",
    "Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized\n",
    "Given context and two different answers , rank each answer based on faithfulness.\n",
    "\n",
    "context:Albert Einstein was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "answer1:Albert Einstein was born in Germany.\n",
    "answer2:Albert Einstein was born in Spain.\n",
    "output:answer1\\n\\nanswer2\n",
    "\n",
    "context:\\n{context}\n",
    "answer1:\\n{answer1}\n",
    "answer2:\\n{answer2}\n",
    "output:\"\"\"\n",
    "\n",
    "\n",
    "context_relevancy_prompt = \"\"\"\n",
    "Context Relevancy measures how relevant retrieved contexts are to the question. Ideally, the context should only contain information necessary to answer the question. The presence of redundant information in the context is penalized.\n",
    "Given an question and context, rank each context based on context relevancy.\n",
    "\n",
    "question:When was Einstein born?\n",
    "context1: Einstein was a German-born theoretical physicist. He was born in 14 March 1879.\n",
    "context2: Albert Einstein was a German-born theoretical physicist. Widely held to be one of the greatest and most influential scientists of all time. He was born in 14 March 1879.\n",
    "output:context1\\n\\ncontext2\n",
    "\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "output:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "answer_relevancy_prompt = \"\"\"\n",
    "Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant information or incomplete answers given a question.\n",
    "Given an question and answer,  rank each answer based on Answer Relevancy.\n",
    "\n",
    "question: Where was Einstein born?\n",
    "answer1:Albert Einstein was born in Germany.\n",
    "answer2:Albert Einstein was born in Spain and lived in America.\n",
    "output:answer1\\n\\nanswer2\n",
    "\n",
    "question:\\n{question}\n",
    "answer1:\\n{answer1}\n",
    "answer2:\\n{answer2}\n",
    "output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "56bec781",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_batch(wikieval_latest.select(range(0,50)), prompt=answer_relevancy_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1702e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [i.split('\\n\\n') for i in results]\n",
    "answer1_ranks = [i.index('answer1') if 'answer1' in i else 0  for i in results]\n",
    "answer2_ranks = [i.index('answer2') if 'answer1' in i else 0  for i in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafc9b3",
   "metadata": {},
   "source": [
    "## Kendall corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4368919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau,spearmanr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "31403661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tau(x,y):\n",
    "    return [kendalltau(x1, y1).statistic for x1,y1 in zip(x,y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f3d36aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.8164965809277261, pvalue=0.12133525035848211)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau([[1.0,0.5],[0.8,0.3]],[[1,0],[1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0fb7a7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5, 0.8, 0.3])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1.0,0.5],[0.8,0.3]]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5c60398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/shahules/Downloads/RAGAS-paper - ragas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e455e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [('answer_faithfulness', 'ungrounded_answer_faithfulness'),\n",
    "        ('context_v1_context_relevancy', 'context_v2_context_relevancy'),\n",
    "       ('answer_answer_relevancy', 'poor_answer_answer_relevancy'),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36b4fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_report(df):\n",
    "    for col in cols:\n",
    "        x = df[col[0]].values.tolist()\n",
    "        y = df[col[1]].values.tolist()\n",
    "        scores = list(zip(x,y))\n",
    "        target = [[1,0]] * 50\n",
    "        print((get_tau(scores,target)))\n",
    "        \n",
    "def accuracy_report(df):\n",
    "    for col in cols:\n",
    "        x = df[col[0]].values.tolist()\n",
    "        y = df[col[1]].values.tolist()\n",
    "        scores = list(zip(x,y))\n",
    "#         scores = [list(x) for x in scores]\n",
    "        scores = [list(np.argsort(x)) if x[0]!=x[1] else [0,0] for x in scores]\n",
    "        target = [[1,0]] * 50\n",
    "        print(sum([s==t for s,t in zip(scores,target)])/len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e45d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26976dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.76\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "accuracy_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip([1,2,3],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4b0f4987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tau([[1,2],[1,3]],[[2,3],[1,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "77985121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=nan, pvalue=nan)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr([[1,2],[1,3]],[[1,2],[1,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050d781",
   "metadata": {},
   "source": [
    "## Checking context relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c206ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"WikiEval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642d35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--WikiEval-33bd2cbc490cc57b/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 256.74it/s]\n"
     ]
    }
   ],
   "source": [
    "wikieval = load_dataset(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb7e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval1 = wikieval.rename_columns({\"context_v1\":\"contexts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e157e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'question', 'contexts', 'context_v2', 'ungrounded_answer', 'source', 'poor_answer'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikieval1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84a317ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column ground_truths not in the dataset. Current columns in the dataset: ['answer', 'question', 'contexts', 'context_v2', 'ungrounded_answer', 'source', 'poor_answer']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwikieval1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcontext_relevancy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/belar/src/ragas/evaluation.py:89\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, column_map)\u001b[0m\n\u001b[1;32m     86\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m [answer_relevancy, context_relevancy, faithfulness, context_recall]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# remap column names from the dataset\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mremap_column_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[1;32m     92\u001b[0m validate_evaluation_modes(dataset, metrics)\n",
      "File \u001b[0;32m~/belar/src/ragas/validation.py:14\u001b[0m, in \u001b[0;36mremap_column_names\u001b[0;34m(dataset, column_map)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mRemap the column names in case dataset uses different column names\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m inverse_column_map \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m column_map\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[0;32m---> 14\u001b[0m     {inverse_column_map[name]: dataset[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_map\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/belar/src/ragas/validation.py:14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mRemap the column names in case dataset uses different column names\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m inverse_column_map \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m column_map\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[0;32m---> 14\u001b[0m     {inverse_column_map[name]: \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_map\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/datasets/arrow_dataset.py:2792\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/datasets/arrow_dataset.py:2776\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2774\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2775\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2776\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2777\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2778\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2779\u001b[0m )\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/datasets/formatting/formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 580\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/datasets/formatting/formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column ground_truths not in the dataset. Current columns in the dataset: ['answer', 'question', 'contexts', 'context_v2', 'ungrounded_answer', 'source', 'poor_answer']\""
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset=wikieval1['train'],metrics=[context_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e92db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
