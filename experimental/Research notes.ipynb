{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4fa2fe",
   "metadata": {},
   "source": [
    "## Log of changes\n",
    "- changes sent tokenizers with pysbd\n",
    "    - think about change as change will affect processing in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db0b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import wikipediaapi\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "openai.api_key = json.load(open(\"/Users/shahules/openai-key.json\"))['ikka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9266b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baaee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You're a bot that answers any given question. If you dont know the exact answer make up one.\"},\n",
    "{\"role\":\"user\", \"content\":\"What were the temperatures and snowfall amounts during the cold snap in Afghanistan in January 2023, and how many people and livestock were affected?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32da92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641c1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Black hole\")\n",
    "\n",
    "\n",
    "def get_page_section(page,chars=8000):\n",
    "    all_text = \"\"\n",
    "    p_wiki = wiki_wiki.page(page)    \n",
    "    return p_wiki.text[:chars]\n",
    "\n",
    "\n",
    "def get_cosine(page, backlinks):\n",
    "    backlinks_vec = model.encode(backlinks)\n",
    "    page_vec = model.encode([page]).reshape(1,-1)\n",
    "    norm = np.linalg.norm(backlinks_vec,axis=1)*np.linalg.norm(page_vec,axis=1)\n",
    "    cosine_sim = np.dot(backlinks_vec,page_vec.T).reshape(-1,)/norm\n",
    "    return cosine_sim\n",
    "\n",
    "def get_backlink_titles(page):\n",
    "    p_wiki = wiki_wiki.page(page) \n",
    "    backlinks =  [i.title() for i in p_wiki.backlinks if \":\" not in i.title()][:100]\n",
    "    if len(backlinks)>1:\n",
    "        c = get_cosine(page, backlinks)\n",
    "        top_indices = c.argsort()[::-1][:10]\n",
    "        return [backlinks[i] for i in top_indices if backlinks[i]!=page]\n",
    "    return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee43d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question_gen = \"\"\"\n",
    "Given a wikipedia text generate a question that can be fully answered from the text of medium difficulty with long form answer seperated by \\n\\n from the given text.\n",
    "\n",
    "Wikipedia Text:\n",
    "The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France. It was named after the engineer Gustave Eiffel, whose company designed and built the structure. Erected in 1889 as the entrance arch to the 1889 World's Fair, it has become a global cultural icon of France and one of the most recognizable structures in the world. The Eiffel Tower is the tallest structure in Paris and the most-visited paid monument in the world; millions of people ascend it every year. The tower stands at a height of 324 meters (1,063 feet) and was the tallest man-made structure in the world until the completion of the Chrysler Building in New York City in 1930.\n",
    "The tower has three levels for visitors, with restaurants on the first and second levels. The third level observatory's upper platform is at 276 meters (906 feet) above the ground, the highest accessible to the public in the European Union. The tower has been featured in numerous films and TV shows, and its lighting is often modified to mark special events or holidays. Despite initial criticism from some of France's leading artists and intellectuals, the Eiffel Tower has become a global cultural icon of France and one of the most recognizable structures in the world.\n",
    "Question: What is the height of the Eiffel Tower, and how does it compare to the height of the Chrysler Building in New York City?\n",
    "\\n\\nAnswer:The height of the Eiffel Tower is 324 meters (1,063 feet). It was the tallest man-made structure in the world until the completion of the Chrysler Building in New York City in 1930. The Chrysler Building surpassed the Eiffel Tower's height and became the new tallest structure, reaching a height of 319 meters (1,046 feet) including its spire. However, it's important to note that the Eiffel Tower remains the tallest structure in Paris and still holds its title as the most-visited paid monument in the world. Despite losing its status as the tallest man-made structure globally, the Eiffel Tower's cultural significance and iconic status endure, attracting millions of visitors each year and leaving an indelible mark on the city of Paris and the world at large.\n",
    "Wikipedia Text:\\n{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850b2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_dataset(pages):\n",
    "    data = []\n",
    "    for page in tqdm(pages):\n",
    "        try:\n",
    "            text = get_page_section(page)\n",
    "            message = [{\"role\": \"user\", \"content\":Question_gen.format(text)}]\n",
    "            output = llm(message)\n",
    "            question, answer_grounded = output['choices'][0]['message']['content'].split(\"\\n\\n\")\n",
    "            message = [{\"role\": \"user\", \"content\":question}]\n",
    "            answer = llm(message)['choices'][0]['message']['content']\n",
    "            data.append({\n",
    "                \"question\":question,\n",
    "                \"grounded_answer\":answer_grounded,\n",
    "                \"answer\":answer,\n",
    "                \"context\":text,\n",
    "                \"source\":page,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6378ff7",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/2023#January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb2ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/shahules/belar/experimental/pages.txt\", \"r\") as file:\n",
    "    pages = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "608f6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = llm(Question_gen.format(c.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aebbacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, answer = output['choices'][0]['message']['content'].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711eca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pages.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64c93114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\"Volcanism on Venus\",\"Pandemic prevention\",\"Jupiter Icy Moons Explorer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ad99e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 3/3 [00:54<00:00, 18.14s/it]\n"
     ]
    }
   ],
   "source": [
    "data_sample = generate_dataset(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf5c68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"/Users/shahules/belar/experimental/ragas_wiki_eval.json\"))\n",
    "data.extend(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49a187f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91b678ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragas-eval-data.json\",'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "919d4860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorn-khiri Sriprachuap\n",
      "Methane synthesis\n",
      "1993–94 BCFC season\n",
      "Hagawi\n",
      "Populus mexicana\n",
      "Gracie Lawrence\n",
      "Gracie Bea Lawrence\n",
      "Laura Cornelius\n",
      "A. D. Macklin\n",
      "Polyosma hirsuta\n",
      "Maltzanella\n",
      "Molina Seca\n",
      "The Life of the Party (play)\n",
      "Ramnagar Assembly constituency (disambiguation)\n",
      "Blue Angel (Park Ji-yoon album\n",
      "A.D. Macklin\n",
      "Blanca Wiethüchter López\n",
      "Frances Hughes\n",
      "Gubad Ibadoghlu\n",
      "2023 BWF World Championships – Men's doubles\n",
      "Jesús Urzagasti Aguilera\n",
      "1925 Reform Party (New Zealand) leadership election\n",
      "Ana Pinho Rodrigues\n",
      "Olympian 6\n",
      "80th Venice Film Festival\n",
      "2023 Venice Film Festival\n",
      "2023 Venice International Film Festival\n",
      "We Find the Bunyip\n",
      "C-ya-laterrrr\n",
      "Evan asano\n",
      "13th Politburo Standing Committee of the Chinese Communist Party\n",
      "Chase Blasi\n",
      "Hateful speech\n",
      "Template:WIR-280\n",
      "Archaeology in Africa\n",
      "Home (Secret Invasion)\n",
      "Anil Thadani\n",
      "Love Me a Little\n",
      "Wayne Stevens (disambiguation)\n",
      "Wayne Osborne (disambiguation)\n",
      "Wayne Hughes (disambiguation)\n",
      "Wayne Hammond (disambiguation)\n",
      "Thai Empire (disambiguation)\n",
      "Oxygen concentration (disambiguation)\n",
      "Danila Kozlov (disambiguation)\n",
      "Taivoan (disambiguation)\n",
      "Salvador Bernal (disambiguation)\n",
      "Gazo (disambiguation)\n",
      "T52 (disambiguation)\n",
      "Sergheevca (disambiguation)\n",
      "Wayne Coffey (disambiguation)\n",
      "Assistant commandant of the Marine Corps\n",
      "Lauren Anne Dickason\n",
      "Lauren Dickason\n",
      "Wikipedia talk:ANVDL\n",
      "Kira Kira, Papua New Guinea\n",
      "Pandemic of 1889-1893\n",
      "BU College of Fine Arts\n",
      "Stephen W. Gard\n",
      "HueUni\n",
      "Oscar Cerruto Collier\n",
      "Matilde Casazola Mendoza\n",
      "In a Room7 F760\n",
      "Zin2 Test5\n",
      "Kirakira (disambiguation)\n",
      "SunTec Group\n",
      "Oxygen hood\n",
      "Francisca Yu-Tsz Chang\n",
      "Athra Alliance\n",
      "Christine Tappolet\n",
      "New Media Writing Prize\n",
      "Mercedes Belzu Gorriti\n",
      "Yolanda Bedregal de Cónitzer\n",
      "Lindaura Anzoátegui de Campero\n",
      "Alcides Arguedas Díaz\n",
      "Supplemental weaving\n",
      "Albanian raid Iranian dissidents\n",
      "Brazil–Libya relations\n",
      "Siegfried Wouthuysen\n",
      "Blackbox Life Recorder 21f / In a Room7 F760\n",
      "West German cinema\n",
      "Leslie Foldy\n",
      "Skorn\n",
      "COZ (disambiguation)\n",
      "CEM (disambiguation)\n",
      "CEP (disambiguation)\n",
      "CAM (disambiguation)\n",
      "Non resistant\n",
      "Nonresistant protest\n",
      "City of Berkeley Landmark\n",
      "Functional textiles\n",
      "Ubilava\n",
      "Category talk:Quebec Cinema Awards navigational boxes\n",
      "Allison Rose\n",
      "María de las Mercedes of Orléans\n",
      "Spinning Globe (song)\n",
      "Spinning Globe\n",
      "Ina Mester\n",
      "Template:Editnotices/Page/Mick Jagger\n",
      "List of Santana band members\n",
      "Sound effect (musical instruments)\n",
      "Sound effect (plays and movies)\n",
      "Mahan (1996 film)\n",
      "2023 Spanish Election\n",
      "Table tennis at the 2023 European Games – Women's singles\n",
      "Spanish election, 2023\n",
      "Spanish election 2023\n",
      "Wenden-Thune-Harxbüttel\n",
      "Thune (Braunschweig)\n",
      "Polaris Building\n",
      "Template:Old good article table entry/doc\n",
      "Sound Affects (album)\n",
      "Pikciute\n",
      "Pikčiutė\n",
      "BIM (disambiguation)\n",
      "Universal Music Russia\n",
      "BIQ (disambiguation)\n",
      "Robin Gollan\n",
      "Murder of Nurhidayati Wartono Surata\n",
      "Golden Dragon Hotel murder\n",
      "Liam Horne\n",
      "Nurhidayati Wartono Surata\n",
      "File talk:Echo & the Bunnymen - The Stars, the Oceans & the Moon.jpg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    get_recent_changes.py\n",
    "\n",
    "    MediaWiki API Demos\n",
    "    Demo of `RecentChanges` module: Get the three most recent changes with\n",
    "    sizes and flags\n",
    "\n",
    "    MIT License\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"format\": \"json\",\n",
    "    \"rcprop\": \"title|timestamp|tags\",\n",
    "    \"list\": \"recentchanges\",\n",
    "    \"action\": \"query\",\n",
    "    \"rclimit\": \"1000\",\n",
    "    \"rctype\":\"new\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "RECENTCHANGES = DATA['query']['recentchanges']\n",
    "remove_tags = [\"User:\",\"Category:\",\"Draft\",\"User talk:\",\"Talk:\",\"Wikipedia:\",\"Template talk:\"]\n",
    "\n",
    "for rc in RECENTCHANGES:\n",
    "    title = str(rc['title'])\n",
    "    timestamp = str(rc['timestamp'])\n",
    "    if not any([x in title for x in remove_tags]) and \"2021\" not in timestamp:\n",
    "        \n",
    "        print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "b074840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"ragas-data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6c20e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evading = [\"I'm sorry\", \"as an AI language model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9499102",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_regen = [item for item in data if any(x.lower() in item['answer'].lower() for x in evading)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ca6c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You're a bot that answers any given question. If you dont know the exact answer make up one.\"},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "47cf1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in topics_regen:\n",
    "    message = [{\"role\": \"system\", \"content\": \"You're a bot that answers any given question. If you dont know the exact answer make up one.\"},]\n",
    "    message.append({\"role\":\"user\",\"content\":item['question']})\n",
    "    output = llm(message)\n",
    "    item['answer'] = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a21cff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 50/50 [01:23<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(data):\n",
    "    titles = get_backlink_titles(item[\"source\"])\n",
    "    context = [get_page_section(item,chars=1000) for item in titles[:2]]\n",
    "    context.insert(0,item['context'])\n",
    "    item[\"context_retrieved\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af204254",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragas_wiki_evalv1.json\",'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67f1fe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/shahules/.cache/huggingface/datasets/json/default-d425d908fb97025a/0.0.0)\n",
      "Pushing dataset shards to the dataset hub:   0%|                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|█████████████████| 1/1 [00:00<00:00, 50.02ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████| 1/1 [00:05<00:00,  5.34s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n"
     ]
    }
   ],
   "source": [
    "Dataset.from_json(\"ragas-eval-data.json\").push_to_hub(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95745ce8",
   "metadata": {},
   "source": [
    "## Generate low relevancy answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307adfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a808a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_answer = \"\"\"\n",
    "Answer the given question partially.\n",
    "Question:{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ea7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--wiki-eval-f0df84e235efd078/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████| 1/1 [00:00<00:00, 89.46it/s]\n",
      "Extracting data files: 100%|████████| 1/1 [00:00<00:00, 389.52it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--wiki-eval-f0df84e235efd078/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 1/1 [00:00<00:00, 224.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1fa267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for item in dataset['train']:\n",
    "    question = item['question']\n",
    "    message = []\n",
    "    message.append({\"role\":\"user\",\"content\":irr_answer.format(question)})\n",
    "    while True:\n",
    "        try:\n",
    "            answer = llm(message)['choices'][0]['message']['content']\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        break\n",
    "    answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b28ee072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].add_column(\"partial_answer\",answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f19512c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%| | 0/1 [00:00<?, ?i\n",
      "Creating parquet from Arrow format: 100%|█| 1/1 [00:00<00:00, 79.01\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                    | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|████████████| 1/1 [00:06<00:00,  6.60s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|█| 1/1 [00:07<00:00\n",
      "Deleting unused files from dataset repository: 100%|█| 1/1 [00:00<0\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "34f604e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ragas_wiki_evalv1.json\",'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c8b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--wiki-eval-80d4ef132547f2df/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 241.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"explodinggradients/wiki-eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601e61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select_columns([\"question\",\"source\",\"answer\",\"grounded_answer\",\"answer_relevancy_v1\",\"context_v1\",\"context_v2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ae0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"answer\":\"ungrounded_answer\",\"answer_relevancy_v1\":\"poor_answer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52b36b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Pushing dataset shards to the dataset hub:   0%|                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|█████████████████| 1/1 [00:00<00:00, 97.16ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████| 1/1 [00:06<00:00,  6.32s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:07<00:00,  7.51s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "Downloading metadata: 100%|████████████████████████| 1.06k/1.06k [00:00<00:00, 3.07MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fe5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def record_score(scores,name,filename='ragas_eli5_score'):\n",
    "    data = json.load(open(f\"{filename}.json\"))\n",
    "    data[name] = scores\n",
    "    with open(f\"{filename}.json\",'w') as file:\n",
    "        json.dump(data,file,indent=4)\n",
    "        \n",
    "def read_score(score_name):\n",
    "    data = json.load(open(\"ragas_eli5_score.json\"))\n",
    "    return data[score_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c141fe",
   "metadata": {},
   "source": [
    "## Ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9ecfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cef7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas.metrics.context_precision import ContextPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185afc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevancy = ContextPrecision(strictness=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc427791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--WikiEval-3b60abf6f625ac40/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.36it/s]\n"
     ]
    }
   ],
   "source": [
    "wikieval = load_dataset(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dd99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval = wikieval['train'].rename_columns({\"grounded_answer\":\"answer\",\"context_v1\":\"contexts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186a4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval1 = wikieval.select_columns(['answer', 'question','contexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736e4e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [04:54<00:00, 73.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 4/4 [08:41<00:00, 130.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [02:01<00:00, 30.46s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset=wikieval1,metrics=[context_relevancy,faithfulness,answer_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ee8159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|█████████████████████| 1/1 [00:00<00:00, 10.41ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87063"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikieval1.select(range(0,20)).to_csv(\"/Users/shahules/Downloads/wikieval_sample20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5abe01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.6130, 'context_relevancy': 0.3521, 'faithfulness': 0.9865, 'answer_relevancy': 0.9613}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f2f901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Answer: The PSLV-C56 mission is scheduled to b...</td>\n",
       "      <td>Question: When is the scheduled launch date an...</td>\n",
       "      <td>[The PSLV-C56 is the 58th mission of Indian Sp...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer: The objective of the Uzbekistan-Afghan...</td>\n",
       "      <td>Question: What is the objective of the Uzbekis...</td>\n",
       "      <td>[The Uzbekistan–Afghanistan–Pakistan Railway P...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer: PharmaCann was founded in 2014 by Theo...</td>\n",
       "      <td>Question: When was PharmaCann founded and what...</td>\n",
       "      <td>[Found in 2014 by Theodore Scott, PharmaCann i...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answer: Christopher Nolan directed the film Op...</td>\n",
       "      <td>Question: Who directed the film Oppenheimer an...</td>\n",
       "      <td>[Oppenheimer is a 2023 biographical thriller f...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Answer: Theranostics, also known as theragnost...</td>\n",
       "      <td>Question: What is theranostics and how does it...</td>\n",
       "      <td>[Theranostics, also known as theragnostics, is...</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Answer: The human climate niche refers to the ...</td>\n",
       "      <td>Question: What is the human climate niche and ...</td>\n",
       "      <td>[The human climate niche is the ensemble of cl...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Answer: Dasypoda radchenkoi belongs to the gen...</td>\n",
       "      <td>Question: What is the taxonomy of Dasypoda rad...</td>\n",
       "      <td>[Dasypoda radchenkoi, also known as Radchenko'...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Answer: The main product of Fremantle Octopus ...</td>\n",
       "      <td>Question: What is the main product of Fremantl...</td>\n",
       "      <td>[Fremantle Octopus is an Australian octopus fi...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Answer: The Managing Director of FoodFutureCo ...</td>\n",
       "      <td>Question: Who is the Managing Director of Food...</td>\n",
       "      <td>[FoodFutureCo is a scale-up accelerator for pu...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Answer: The purpose of designing and building ...</td>\n",
       "      <td>Question: What was the purpose of designing an...</td>\n",
       "      <td>[The Fiat Ecobasic is a concept car designed b...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Answer: The Rainbow Plaque programme in the UK...</td>\n",
       "      <td>Question: What is the purpose of the Rainbow P...</td>\n",
       "      <td>[The Rainbow Plaque programme is a UK scheme i...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Answer: The Zubaydah Trail, also known as the ...</td>\n",
       "      <td>Question: What is the Zubaydah Trail and when ...</td>\n",
       "      <td>[The Zubaydah Trail (Al-Kufi pilgrimage route)...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Answer: The Chimnabai Clock Tower was complete...</td>\n",
       "      <td>Question: When was the Chimnabai Clock Tower c...</td>\n",
       "      <td>[The Chimnabai Clock Tower, also known as the ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Answer: Trolleybus Route 20 in Shanghai starte...</td>\n",
       "      <td>Question: When did Trolleybus Route 20 in Shan...</td>\n",
       "      <td>[Trolleybus Route 20 is a trolleybus route in ...</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Answer: The Inter Expo Center in Sofia, Bulgar...</td>\n",
       "      <td>Question: When did the Inter Expo Center in So...</td>\n",
       "      <td>[The Inter Expo Center (IEC) is a multi-purpos...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Answer: Pope Benedict XVI became the head of t...</td>\n",
       "      <td>Question: When did Pope Benedict XVI become th...</td>\n",
       "      <td>[Pope Benedict XVI (Latin: Benedictus PP. XVI;...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Answer: The crash of Yeti Airlines Flight 691 ...</td>\n",
       "      <td>Question: What caused the crash of Yeti Airlin...</td>\n",
       "      <td>[Yeti Airlines Flight 691 was a scheduled dome...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Answer: The Starship rocket has a height of 12...</td>\n",
       "      <td>Question: How does the height and thrust of th...</td>\n",
       "      <td>[Starship is a super heavy-lift launch vehicle...</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Answer: The Kyzylkum Desert is known for its d...</td>\n",
       "      <td>Question: What is the Kyzylkum Desert known fo...</td>\n",
       "      <td>[The Kyzylkum Desert (Uzbek: Qizilqum, Қизилқу...</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Answer: The 80th annual Venice International F...</td>\n",
       "      <td>Question: When will the 80th annual Venice Int...</td>\n",
       "      <td>[The 80th annual Venice International Film Fes...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Answer: Myosotis angustata is endemic to the S...</td>\n",
       "      <td>Question: Where is Myosotis angustata endemic ...</td>\n",
       "      <td>[Myosotis angustata is a species of flowering ...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Answer: Mount Brown is located in Liberty Coun...</td>\n",
       "      <td>Question: Where is Mount Brown located and wha...</td>\n",
       "      <td>[Mount Brown is a 6,958-foot-elevation (2,121-...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Answer: The type locality of the Laoshan tree ...</td>\n",
       "      <td>Question: Where is the type locality of the La...</td>\n",
       "      <td>[The Laoshan tree frog (Rhacophorus laoshan) i...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Answer: The Roanoke and Tar River Railroad was...</td>\n",
       "      <td>Question: When was the Roanoke and Tar River R...</td>\n",
       "      <td>[The Roanoke and Tar River Railroad was a rail...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Answer: Moud Goba is one of the founding membe...</td>\n",
       "      <td>Question: What organizations has Moud Goba bee...</td>\n",
       "      <td>[Moud Goba is a Zimbabwean LGBTIQ+ human right...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Answer: The purpose of the Modernizing Opioid ...</td>\n",
       "      <td>Question: What is the purpose of the Modernizi...</td>\n",
       "      <td>[The Modernizing Opioid Treatment Access Act i...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Answer: Gaucho Americano had its world premier...</td>\n",
       "      <td>Question: When and where did Gaucho Americano ...</td>\n",
       "      <td>[Gaucho Americano (lit. 'American Gaucho') is ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Answer: The type locality of the Blakistonia p...</td>\n",
       "      <td>Question: Where is the type locality of the Bl...</td>\n",
       "      <td>[Blakistonia plata is a species of mygalomorph...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Answer: In the 2022–23 season, the Turkish Wom...</td>\n",
       "      <td>Question: How many teams participate in the Tu...</td>\n",
       "      <td>[The Turkish Women's Football Super League (Tu...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Answer: The Tomb of Alexander Stewart, also kn...</td>\n",
       "      <td>Question: Who is buried in the Tomb of Alexand...</td>\n",
       "      <td>[The Tomb of Alexander Stewart (or Tomb of the...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Answer: The 5th Separate Guards Tatsin Red Ban...</td>\n",
       "      <td>Question: When was the 5th Separate Guards Tat...</td>\n",
       "      <td>[The 5th Separate Guards Tatsin Red Banner Ord...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Answer: The Siege of Mariupol began on 24 Febr...</td>\n",
       "      <td>Question: How long did the Siege of Mariupol l...</td>\n",
       "      <td>[The Siege of Mariupol began on 24 February 20...</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Answer: A large number of countries, including...</td>\n",
       "      <td>Question: Which countries and international or...</td>\n",
       "      <td>[International sanctions have been imposed aga...</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Answer: The Sri Lankan economic crisis was cau...</td>\n",
       "      <td>Question: What factors contributed to the Sri ...</td>\n",
       "      <td>[The Sri Lankan economic crisis is an ongoing ...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Answer: The 2022 Hormozgan earthquakes were a ...</td>\n",
       "      <td>Question: How many people were killed and inju...</td>\n",
       "      <td>[The 2022 Hormozgan earthquakes were a pair of...</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Answer: Seven people were killed and 48 others...</td>\n",
       "      <td>Question: How many people were killed and inju...</td>\n",
       "      <td>[On July 4, 2022, a mass shooting occurred dur...</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Answer: Uber has been involved in a number of ...</td>\n",
       "      <td>Question: What are some of the controversies s...</td>\n",
       "      <td>[Uber Technologies, Inc.  or Uber has been the...</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Answer: Initial estimates were that up to four...</td>\n",
       "      <td>Question: What was the estimated timeline for ...</td>\n",
       "      <td>[On December 3, 2022, a shooting attack was ca...</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Answer: The ethnic violence in Manipur in 2023...</td>\n",
       "      <td>Question: What caused the ethnic violence in M...</td>\n",
       "      <td>[On 3 May 2023, ethnic violence erupted in Ind...</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Answer: The Chinese balloon that was spotted i...</td>\n",
       "      <td>Question: What was the size and payload of the...</td>\n",
       "      <td>[From January 28 to February 4, 2023, a high-a...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Answer: During the cold snap in Afghanistan in...</td>\n",
       "      <td>Question: What were the temperatures and snowf...</td>\n",
       "      <td>[A cold snap began in Afghanistan on January 1...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Answer: GPT-4 was released on March 14, 2023. ...</td>\n",
       "      <td>Question: When was GPT-4 released and what are...</td>\n",
       "      <td>[Generative Pre-trained Transformer 4 (GPT-4) ...</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Answer: As of September 2022, the Myanmar civi...</td>\n",
       "      <td>Question: What is the current status of the My...</td>\n",
       "      <td>[The Myanmar civil war (Burmese: ၂၀၂၁-၂၀၂၃ မြန...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.941080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Answer: The Miss Grand Dominican Republic 2023...</td>\n",
       "      <td>Question: When and where will the Miss Grand D...</td>\n",
       "      <td>[Miss Grand Dominican Republic 2023 will be th...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Answer: The rebellion staged by the Wagner Gro...</td>\n",
       "      <td>Question: What was the cause of the rebellion ...</td>\n",
       "      <td>[On 23 June 2023, the Wagner Group, a Russian ...</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Answer: The gas supply outage in Sheffield, En...</td>\n",
       "      <td>Question: What caused the gas supply outage in...</td>\n",
       "      <td>[The city of Sheffield, England was impacted b...</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Answer: The civil unrest and protests in Iran ...</td>\n",
       "      <td>Question: What sparked the civil unrest and pr...</td>\n",
       "      <td>[Civil unrest and protests against the governm...</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Answer: On Venus, there are shield volcanoes, ...</td>\n",
       "      <td>Question: What types of volcanoes are found on...</td>\n",
       "      <td>[The surface of Venus is dominated by volcanic...</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.966426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Answer: Some measures for pandemic prevention ...</td>\n",
       "      <td>Question: What are some measures for pandemic ...</td>\n",
       "      <td>[Pandemic prevention is the organization and m...</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Answer: The main science objectives of the JUI...</td>\n",
       "      <td>Question: What are the main science objectives...</td>\n",
       "      <td>[The JUpiter ICy moons Explorer (Juice, former...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.953931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer  ... answer_relevancy\n",
       "0   Answer: The PSLV-C56 mission is scheduled to b...  ...         0.942336\n",
       "1   Answer: The objective of the Uzbekistan-Afghan...  ...         0.949762\n",
       "2   Answer: PharmaCann was founded in 2014 by Theo...  ...         0.963253\n",
       "3   Answer: Christopher Nolan directed the film Op...  ...         0.984808\n",
       "4   Answer: Theranostics, also known as theragnost...  ...         0.981180\n",
       "5   Answer: The human climate niche refers to the ...  ...         0.981146\n",
       "6   Answer: Dasypoda radchenkoi belongs to the gen...  ...         0.969072\n",
       "7   Answer: The main product of Fremantle Octopus ...  ...         0.971285\n",
       "8   Answer: The Managing Director of FoodFutureCo ...  ...         0.977580\n",
       "9   Answer: The purpose of designing and building ...  ...         0.981272\n",
       "10  Answer: The Rainbow Plaque programme in the UK...  ...         0.961344\n",
       "11  Answer: The Zubaydah Trail, also known as the ...  ...         0.921753\n",
       "12  Answer: The Chimnabai Clock Tower was complete...  ...         0.978963\n",
       "13  Answer: Trolleybus Route 20 in Shanghai starte...  ...         0.986743\n",
       "14  Answer: The Inter Expo Center in Sofia, Bulgar...  ...         0.965279\n",
       "15  Answer: Pope Benedict XVI became the head of t...  ...         0.984271\n",
       "16  Answer: The crash of Yeti Airlines Flight 691 ...  ...         0.974167\n",
       "17  Answer: The Starship rocket has a height of 12...  ...         0.931076\n",
       "18  Answer: The Kyzylkum Desert is known for its d...  ...         0.970542\n",
       "19  Answer: The 80th annual Venice International F...  ...         0.961350\n",
       "20  Answer: Myosotis angustata is endemic to the S...  ...         0.956137\n",
       "21  Answer: Mount Brown is located in Liberty Coun...  ...         0.984749\n",
       "22  Answer: The type locality of the Laoshan tree ...  ...         0.951691\n",
       "23  Answer: The Roanoke and Tar River Railroad was...  ...         0.961500\n",
       "24  Answer: Moud Goba is one of the founding membe...  ...         0.936848\n",
       "25  Answer: The purpose of the Modernizing Opioid ...  ...         0.972466\n",
       "26  Answer: Gaucho Americano had its world premier...  ...         0.971873\n",
       "27  Answer: The type locality of the Blakistonia p...  ...         0.969761\n",
       "28  Answer: In the 2022–23 season, the Turkish Wom...  ...         0.945319\n",
       "29  Answer: The Tomb of Alexander Stewart, also kn...  ...         0.971063\n",
       "30  Answer: The 5th Separate Guards Tatsin Red Ban...  ...         0.990852\n",
       "31  Answer: The Siege of Mariupol began on 24 Febr...  ...         0.941023\n",
       "32  Answer: A large number of countries, including...  ...         0.921415\n",
       "33  Answer: The Sri Lankan economic crisis was cau...  ...         0.979397\n",
       "34  Answer: The 2022 Hormozgan earthquakes were a ...  ...         0.917789\n",
       "35  Answer: Seven people were killed and 48 others...  ...         0.915272\n",
       "36  Answer: Uber has been involved in a number of ...  ...         0.971675\n",
       "37  Answer: Initial estimates were that up to four...  ...         0.902676\n",
       "38  Answer: The ethnic violence in Manipur in 2023...  ...         0.975990\n",
       "39  Answer: The Chinese balloon that was spotted i...  ...         0.963874\n",
       "40  Answer: During the cold snap in Afghanistan in...  ...         0.937017\n",
       "41  Answer: GPT-4 was released on March 14, 2023. ...  ...         0.979541\n",
       "42  Answer: As of September 2022, the Myanmar civi...  ...         0.941080\n",
       "43  Answer: The Miss Grand Dominican Republic 2023...  ...         0.966655\n",
       "44  Answer: The rebellion staged by the Wagner Gro...  ...         0.985703\n",
       "45  Answer: The gas supply outage in Sheffield, En...  ...         0.984112\n",
       "46  Answer: The civil unrest and protests in Iran ...  ...         0.935236\n",
       "47  Answer: On Venus, there are shield volcanoes, ...  ...         0.966426\n",
       "48  Answer: Some measures for pandemic prevention ...  ...         0.978360\n",
       "49  Answer: The main science objectives of the JUI...  ...         0.953931\n",
       "\n",
       "[50 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4c023",
   "metadata": {},
   "source": [
    "### Concat reannoated samples with all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f7bb2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--WikiEval-3b60abf6f625ac40/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 243.18it/s]\n",
      "Found cached dataset csv (/Users/shahules/.cache/huggingface/datasets/csv/default-7607020e99f8dfe0/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/shahules/.cache/huggingface/datasets/csv/default-6323ab1c8fd55177/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████| 1/1 [00:00<00:00, 1953.56it/s]\n",
      "Extracting data files: 100%|█████████████████████████████| 1/1 [00:00<00:00, 204.24it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/shahules/.cache/huggingface/datasets/csv/default-6323ab1c8fd55177/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "wikieval_og = load_dataset(\"explodinggradients/WikiEval\")\n",
    "wikieval_local = Dataset.from_csv(\"/Users/shahules/Downloads/wikieval_sample20 - wikieval_sample20.csv.csv\")\n",
    "wikieval_poor_answer = Dataset.from_csv(\"/Users/shahules/Downloads/wikieval_sample20 - poor_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a07605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_og = wikieval_og.rename_columns({\"grounded_answer\":\"answer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "016668cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval = wikieval_og['train'].select(range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd553008",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_local = wikieval_local.add_column('poor_answer', wikieval['poor_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b6ba878",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_local = wikieval_local.add_column('source', wikieval['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4dae604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/csv/default-7607020e99f8dfe0/0.0.0/cache-b666c3bbfd9a9a68.arrow\n",
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/csv/default-7607020e99f8dfe0/0.0.0/cache-51de8ac189b2e2d9.arrow\n"
     ]
    }
   ],
   "source": [
    "wikieval_local = wikieval_local.rename_columns({'contexts':'context_v1'})\n",
    "wikieval_local = wikieval_local.map(lambda x : {\"context_v1\":eval(x['context_v1'])})\n",
    "wikieval_local = wikieval_local.map(lambda x : {\"context_v2\":eval(x['context_v2'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2797e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43efb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_latest = concatenate_datasets([wikieval_local,wikieval_og['train'].select(range(20,50))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeef27e",
   "metadata": {},
   "source": [
    "- Update poor answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fca483c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_latest = wikieval_latest.remove_columns(['poor_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26237eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval_latest = wikieval_latest.add_column('poor_answer',wikieval_poor_answer['poor_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa7efd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%|                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|█████████████████| 1/1 [00:00<00:00, 73.66ba/s]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:08<00:00,  8.11s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "wikieval_latest.push_to_hub(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc73af",
   "metadata": {},
   "source": [
    "## Eli5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6e9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "eli5_ragas = pickle.load(open(\"eli5_ragas.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06e8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5_ragas = Dataset.from_list(eli5_ragas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab7ead",
   "metadata": {},
   "source": [
    "## reannotates sample\n",
    "Test assumptions\n",
    "1. Ragas scores ranked correctly\n",
    "2. GPT3.5 scores \n",
    "3. GPT3.5 rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cdc498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas.metrics.answer_relevance import AnswerRelevancy\n",
    "# from ragas.metrics.context_relevance import sent_tokenize as ragas_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad90db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f5ddc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy=AnswerRelevancy(embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd90b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5_ragas = eli5_ragas.select(range(0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14ab2b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'answer', 'id', 'poor_answer', 'ungrounded_answer', 'context_v2'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5_ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65b5548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stallman does not like the term \"Open Source\" because he believes it does not emphasize the freedom issues associated with software. He fears that the ideals of freedom and community are threatened by compromising on the Free Software Foundation\\'s idealistic standards for software freedom. Stallman prefers the term \"Free Software\" because he sees it as a political movement, while he views open source as merely a development model. He believes that the use of the term \"Open Source\" will not lead to people valuing and defending their freedom.',\n",
       " 'The Soviets both innovated and used designs obtained through espionage from Nazi Germany and the Western nations.',\n",
       " 'Computer code can be \"secret\" even when you have a copy of it on your computer through various forms of protection and encryption. For example, the code may be compressed or encrypted in a way that makes it unreadable without a specific decryption algorithm. Additionally, certain programs or files may have protections in place that prevent them from being copied or transferred, or they may only function on a specific computer or operating system. Even if you have a copy of the code, without the necessary tools or permissions to access and understand it, the code remains \"secret\".',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " \"A REAL ID is a type of identification that complies with the Real ID Act of 2005 set by the Department of Homeland Security. It is different from a standard driver's license as it requires more documents for verification, including proof of residency, citizenship or legal immigrant status, and Social Security number. A REAL ID can be used for federal purposes such as air travel. Some states offer a multi-tier system offering standard licenses, REAL ID licenses, and enhanced licenses. The enhanced license is also REAL ID compliant and provides proof of U.S. citizenship.\",\n",
       " \"The Earth has magnetic poles due to the motion of fluid in the Earth's outer core. This motion creates a magnetic field, which is represented by the magnetic poles. The theoretical model that best accounts for this magnetic field is a powerful bar magnet at the center of the Earth. The actual magnetic poles are constantly moving due to the fluid motion in the Earth's outer core.\",\n",
       " 'Insufficient information.',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'If you get the wrong blood type, your body may produce extra antibodies, which can lead to symptoms that may be mistaken for other conditions. In severe cases, it can cause serious complications or even death.',\n",
       " 'The context suggests that nations pursue economic growth over economic balance because periods of economic growth are characterized by some form of symbiotic imbalance. The focus should be on managing the distortions caused by the imbalance rather than forcing a return to some preconceived equilibrium. The next round of global growth will only be sustained if the rest of the world can manage these distortions. The context also suggests that productivity measures are key indicators of economic performance and a key source of economic growth and competitiveness. Therefore, nations might pursue economic growth to improve their productivity and competitiveness. As for the question of whether perpetual growth is cancerous, the context suggests that there are indeed challenges associated with perpetual economic growth, such as the depletion of high-quality fossil fuel resources. However, it does not definitively state that perpetual growth is cancerous.',\n",
       " 'The context mentions several reasons why dogs or cats might be put down. These include severe health issues such as obesity-related complications, diseases affecting the mouth and urinary tract, and skin cancer. Additionally, animals might be euthanized for financial reasons, as mentioned in the context of \"mercy-farms\" in Germany.',\n",
       " 'Insufficient information.',\n",
       " \"Early films/photos were in black and white because the technology and processes for producing color were not yet developed or were too expensive. To see color, color film processes were needed, which included the use of dyes and tinted film bases. However, these were substantially more expensive than black and white processes. It wasn't until cheaper color processes were introduced in the 1950s that color films became more common.\",\n",
       " 'Solid water is less dense than liquid water because as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached, hydrogen bonding dominates the intermolecular forces, which results in a packing of molecules less compact in the solid. This is due to the reduction of thermal motion with cooling, which allows water molecules to form more hydrogen bonds that prevent the molecules from coming close to each other.',\n",
       " 'Insufficient information',\n",
       " 'Fire investigators, who are experienced firefighters trained in fire cause determinism, are dispatched to fire scenes to investigate and determine whether the fire was a result of an accident or intentional. They may have full law enforcement powers to investigate and arrest suspected arsonists. They work to determine the origin and cause of a fire, often working with local, state and federal investigators. They may also work with agents from the Bureau of Alcohol, Tobacco, Firearms and Explosives (ATF). The investigation process may involve looking for tangible clues, interviewing witnesses, and examining the scene for possible indicators of the cause. However, in some cases, if the fire has destroyed possible indicators, the cause may not be determined with certainty.',\n",
       " 'Fire is a process of combustion, often producing heat, light, and other chemical reactions. In the context provided, it refers to an uncontrolled fire in an area of combustible vegetation occurring in rural areas. It can also refer to a fire that needs to be extinguished in an emergency situation.',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'A secret recipe stays secret through a mutually agreed-upon construct when speaking with outside members. Agreement to maintain the secret is often coerced through \"shaming\" and reference to family honor. The information may even be something as trivial as a recipe. Secrets are sometimes kept to provide the pleasure of surprise. This includes keeping secret about a surprise party, not telling spoilers of a story, and avoiding exposure of a magic trick. Keeping one\\'s strategy secret is important in many aspects of game theory.',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'Your cell phone figures out your current location using a feature that uses the GPS / Assisted GPS location of the mobile device, if available, supplemented by determining the nearest wireless networks and cell sites. The software looks up the location of the cell site using a database of known wireless networks and sites. By triangulating the different signal strengths from cell transmitters and then using their location property (retrieved from the database), it determines your current location. The cell-phone service provider gets the location from a GPS chip built into the phone, or using radiolocation and trilateration based on the signal-strength of the closest cell-phone towers (for phones without GPS features).',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'There are many universal humanitarian laws applying to war. For instance, it is illegal for a combatant specifically to target civilians and certain types of weapons that cause indiscriminate damage are categorically outlawed. All states seem to observe these rules, making them a part of customary international law. Incentives for a nation to adhere to these rules could include maintaining international relations and avoiding sanctions or retaliation.',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'The context does not provide specific information on why \"Organic\" food is more expensive at the grocery stores.',\n",
       " 'When a file gets compressed, its size is reduced due to the nature of compression algorithms. The file is stored either as plain text or as compressed text in a ZIP archive format. The compression can be done manually using compression software or automatically by the exporting software during write. When data is compressed, its entropy increases, and it cannot increase indefinitely. Most compression algorithms can recognize when further compression would be pointless and would in fact increase the size of the data. The compressed file is smaller than the original file, but repeatedly compressing the same file will not reduce the size to nothing.',\n",
       " 'The context does not provide specific information on what happens to the .1 percent of bacteria not killed by antibacterial hand soap, what they are, or if they could be dangerous.',\n",
       " 'Remastering a movie involves recreating the film with enhancements or improvements. This could include increasing the resolution, as in the case of Stouffer\\'s attempt to remaster Wild America in 4K, or it could involve color correction, sharpening lines, or other visual improvements. However, the process can be controversial, with some critics describing it as \"digital ruination\" or \"digital destruction.\"',\n",
       " 'Irrational numbers, including pi, do not end because they cannot be expressed as a ratio of two integers. This means they have an infinite number of digits in their decimal representation, and they do not settle into an infinitely repeating pattern of digits.',\n",
       " 'Insufficient information',\n",
       " 'A compiler is created by taking a description of a formal grammar of a specific programming language and producing a parser for that language. This parser detects and identifies the reserved words and symbols of the specific language from a stream of text and returns these as tokens to the code which implements the syntactic validation and translation into object code. This second part of the compiler can also be created by a compiler-compiler. The compiler output is typically assembly language, which is then processed through a peep-hole optimizer before the assembler and linker.',\n",
       " 'The chalk at the billiards table is used for the cue tip. It can significantly affect play and its effectiveness can be impaired by high humidity. Harder, drier compounds are generally considered superior by most players.',\n",
       " 'Insufficient information',\n",
       " 'Cloudy hot water can be due to tiny air bubbles getting trapped in the water. When water is heated or frozen quickly, dissolved gases can no longer stay in solution and come out as microscopic bubbles, causing the water to appear cloudy.',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " 'Insufficient information',\n",
       " \"Random numbers are generated by a computer through a set of equations, producing what are known as pseudo-random numbers. These numbers are not truly random in the strictest sense. A random number generator is a device that produces a sequence of numbers which cannot be easily identified with deterministic properties. This sequence is then called a sequence of stochastic numbers. The algorithms typically rely on pseudorandom numbers, computer generated numbers mimicking true random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by an initial value, called the PRNG's seed. Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility.\",\n",
       " \"Making smaller transistors in processors contributes to improving the processor overall by allowing for more transistors to fit on a single chip, which can increase processing power. Additionally, the 'extra space' that the transistor shrinkage provides allows processor manufacturers to add specialized processing units to deal with features such as graphics, video, and cryptography.\",\n",
       " \"Countries that print their own currency, like the USA, can technically create more money to pay off their debt. However, this can lead to inflation or hyperinflation if done excessively, as it devalues the currency. This is why people are concerned about it. Additionally, a large part of a country's debt is often held by its own financial institutions, like banks and pension funds. Restructuring this debt could undermine the health of the domestic financial system. Furthermore, debt issuance often surpasses equity issuance in currency value, and when debt matures, new debt is often issued to repay the old debt, potentially leading to a cycle of increasing debt.\",\n",
       " 'The exact reason why hot water can freeze faster than cold water (known as the Mpemba effect) is not clearly defined and there is disagreement about the parameters required to produce the effect and about its theoretical basis. Some suggest it could be due to a particle velocity distribution function that significantly deviates from the Maxwell-Boltzmann distribution. Others suggest it could be due to non-linear effects that influence the freezing process. However, the phenomenon is difficult to reproduce or confirm and it is not clear whether the explanation would be trivial or illuminating.',\n",
       " 'Insufficient information']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5_ragas['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e5d3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    \"question\":\"question\",\n",
    "    \"answer\":\"answer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d38f5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [03:58<00:00, 59.70s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset=eli5_ragas,\n",
    "                   metrics=[context_relevancy],column_map=column_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0411a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.2292, 'faithfulness': 0.8634, 'answer_relevancy': 0.8269, 'context_precision': 0.0933}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c473e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_score(results.to_pandas()['context_precision'].values.tolist(),\"context_v2_context_relevancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2049ca75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['grounded_faithfulness', 'answer_answer_relevancy', 'contexts_context_precision', 'ungrounded_answer_faithfulness', 'poor_answer_answer_relevancy', 'context_v2_context_relevancy'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open(\"ragas_eli5_score.json\")).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7a3b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_matrix = np.vstack([read_score(\"contexts_context_precision\"),read_score(\"context_v2_context_relevancy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb7b4031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(comp_matrix[0,:] >= comp_matrix[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bee95b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3, 11]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(comp_matrix[0,:] < comp_matrix[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdabf8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ragas_tokenizer(wikieval1[1]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a51d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9c4e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(wikieval1[1]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33d329ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pysbd\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "len(seg.segment(wikieval1[1]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee453b",
   "metadata": {},
   "source": [
    "#### Annotate samples using gpt direct scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17f3e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"answer_relevancy\"\n",
    "definition = \"\"\"answer relevancy: refers to the degree to which a response directly addresses and is appropriate for a given question or context. Penalize incomplete or reduntant information in the answer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96257747",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "{definition}\n",
    "Given question and answer, assign a score for {metric_name} in the range 0-10.\n",
    "question:\\n{question}\n",
    "answer:\\n{answer}\n",
    "{metric_name} score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f1d841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = wikieval[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a47d61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = prompt.format(definition=definition,metric_name=metric_name,\n",
    "                             question=item['question'],\n",
    "                             answer=item['grounded_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1260be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7yHqQgOC2q8W4x876LorxX9Im4bP3 at 0x7f78d1192570> JSON: {\n",
       "  \"id\": \"chatcmpl-7yHqQgOC2q8W4x876LorxX9Im4bP3\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1694602630,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"8\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 115,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 116\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e146a9",
   "metadata": {},
   "source": [
    "### Annotate using gpt ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80597f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "{definition}\n",
    "Given question and two contexts , output rank of each context based on {metric_name}\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "output ranks:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c97b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20bb12c3",
   "metadata": {},
   "source": [
    "## Run in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfbaee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.metrics.llms import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0eaf9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =  ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "40943ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(dataset,prompt,llm,batch_size=15):\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0,len(dataset),batch_size):\n",
    "        prompts = []\n",
    "        ds = dataset.select(range(i,min(len(dataset),i+batch_size)))\n",
    "        answer1, answer2, question = ds[\"answer\"], ds[\"poor_answer\"], ds[\"question\"]\n",
    "        for q, a1, a2 in zip(question, answer1, answer2):\n",
    "#                 c_str: str = \"\\n\".join(c)\n",
    "                human_prompt = prompt.format(answer1=a2, answer2=a1, question=q)\n",
    "                prompts.append(ChatPromptTemplate.from_messages([human_prompt]))  \n",
    "        result = generate(prompts, llm)\n",
    "        result = [output[0].text for output in result.generations]\n",
    "        results.extend(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c2b39",
   "metadata": {},
   "source": [
    "## prompt scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2aa3cd",
   "metadata": {},
   "source": [
    "`<column_name>_<metric_name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "21e97941",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt = \"\"\"\n",
    "Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized\n",
    "Given an answer and context, assign a score for faithfulness in the range 0-10.\n",
    "\n",
    "answer:\\n{answer}\n",
    "context:\\n{context}\n",
    "faithfulness score:\n",
    "\"\"\"\n",
    "\n",
    "context_relevancy_prompt = \"\"\"\n",
    "Context Relevancy measures how relevant retrieved contexts are to the question. Ideally, the context should only contain information necessary to answer the question. The presence of redundant information in the context is penalized.\n",
    "Given an question and context, assign a score for context relevancy in the range 0-10.\n",
    "\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "context relevancy score:\n",
    "\"\"\"\n",
    "\n",
    "answer_relevancy_prompt = \"\"\"\n",
    "Answer Relevanc measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant information or incomplete answers given a question.\n",
    "Given an question and answer, assign a score for answer relevancy in the range 0-10.\n",
    "\n",
    "question:\\n{question}\n",
    "answer:\\n{answer}\n",
    "answer relevancy score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ebb9e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_batch(wikieval_latest.select(range(0,50)), prompt=faithfulness_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f08067ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [float(i) for i in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f3da2",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33889d8f",
   "metadata": {},
   "source": [
    "`We always provide the poor answer first and best answer last `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "235bd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt = \"\"\"\n",
    "Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized\n",
    "Given context and two different answers , rank each answer based on faithfulness.\n",
    "\n",
    "context:Albert Einstein was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n",
    "answer1:Albert Einstein was born in Germany.\n",
    "answer2:Albert Einstein was born in Spain.\n",
    "output:answer1\\n\\nanswer2\n",
    "\n",
    "context:\\n{context}\n",
    "answer1:\\n{answer1}\n",
    "answer2:\\n{answer2}\n",
    "output:\"\"\"\n",
    "\n",
    "\n",
    "context_relevancy_prompt = \"\"\"\n",
    "Context Relevancy measures how relevant retrieved contexts are to the question. Ideally, the context should only contain information necessary to answer the question. The presence of redundant information in the context is penalized.\n",
    "Given an question and context, rank each context based on context relevancy.\n",
    "\n",
    "question:When was Einstein born?\n",
    "context1: Einstein was a German-born theoretical physicist. He was born in 14 March 1879.\n",
    "context2: Albert Einstein was a German-born theoretical physicist. Widely held to be one of the greatest and most influential scientists of all time. He was born in 14 March 1879.\n",
    "output:context1\\n\\ncontext2\n",
    "\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "output:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "answer_relevancy_prompt = \"\"\"\n",
    "Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant information or incomplete answers given a question.\n",
    "Given an question and answer,  rank each answer based on Answer Relevancy.\n",
    "\n",
    "question: Where was Einstein born?\n",
    "answer1:Albert Einstein was born in Germany.\n",
    "answer2:Albert Einstein was born in Spain and lived in America.\n",
    "output:answer1\\n\\nanswer2\n",
    "\n",
    "question:\\n{question}\n",
    "answer1:\\n{answer1}\n",
    "answer2:\\n{answer2}\n",
    "output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "56bec781",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_batch(wikieval_latest.select(range(0,50)), prompt=answer_relevancy_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1702e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [i.split('\\n\\n') for i in results]\n",
    "answer1_ranks = [i.index('answer1') if 'answer1' in i else 0  for i in results]\n",
    "answer2_ranks = [i.index('answer2') if 'answer1' in i else 0  for i in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafc9b3",
   "metadata": {},
   "source": [
    "## Kendall corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4368919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau,spearmanr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "31403661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tau(x,y):\n",
    "    return [kendalltau(x1, y1).statistic for x1,y1 in zip(x,y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f3d36aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.8164965809277261, pvalue=0.12133525035848211)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendalltau([[1.0,0.5],[0.8,0.3]],[[1,0],[1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0fb7a7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5, 0.8, 0.3])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1.0,0.5],[0.8,0.3]]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5c60398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/shahules/Downloads/RAGAS-paper - ragas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e455e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [('answer_faithfulness', 'ungrounded_answer_faithfulness'),\n",
    "        ('context_v1_context_relevancy', 'context_v2_context_relevancy'),\n",
    "       ('answer_answer_relevancy', 'poor_answer_answer_relevancy'),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36b4fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_report(df):\n",
    "    for col in cols:\n",
    "        x = df[col[0]].values.tolist()\n",
    "        y = df[col[1]].values.tolist()\n",
    "        scores = list(zip(x,y))\n",
    "        target = [[1,0]] * 50\n",
    "        print((get_tau(scores,target)))\n",
    "        \n",
    "def accuracy_report(df):\n",
    "    for col in cols:\n",
    "        x = df[col[0]].values.tolist()\n",
    "        y = df[col[1]].values.tolist()\n",
    "        scores = list(zip(x,y))\n",
    "#         scores = [list(x) for x in scores]\n",
    "        scores = [list(np.argsort(x)) if x[0]!=x[1] else [0,0] for x in scores]\n",
    "        target = [[1,0]] * 50\n",
    "        print(sum([s==t for s,t in zip(scores,target)])/len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e45d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26976dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.76\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "accuracy_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip([1,2,3],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4b0f4987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tau([[1,2],[1,3]],[[2,3],[1,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "77985121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=nan, pvalue=nan)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr([[1,2],[1,3]],[[1,2],[1,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3999c",
   "metadata": {},
   "source": [
    "## Checking context relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88112b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"WikiEval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642d35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--WikiEval-33bd2cbc490cc57b/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 201.46it/s]\n"
     ]
    }
   ],
   "source": [
    "wikieval = load_dataset(\"explodinggradients/WikiEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1335e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikieval1 = wikieval.rename_columns({\"context_v2\":\"contexts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a585642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'question', 'context_v1', 'contexts', 'ungrounded_answer', 'source', 'poor_answer'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikieval1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "145dfa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 4/4 [05:35<00:00, 83.92s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset=wikieval1['train'],metrics=[context_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c0bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_badv2.pkl','wb') as obj:\n",
    "    pickle.dump(results, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af59fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_good = pickle.load(open(\"results_goodv2.pkl\",\"rb\"))\n",
    "results_bad = pickle.load(open(\"results_badv2.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7130fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_good_df = results_good.to_pandas()\n",
    "results_bad_df = results_bad.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d424237",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = results_good_df[(results_good_df['context_precision'] - results_bad_df['context_precision'])<0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6235663c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "640aee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Question: What is the Zubaydah Trail and when ...</td>\n",
       "      <td>[The Zubaydah Trail (Al-Kufi pilgrimage route)...</td>\n",
       "      <td>Answer: The Zubaydah Trail, also known as the ...</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Question: Who is buried in the Tomb of Alexand...</td>\n",
       "      <td>[The Tomb of Alexander Stewart (or Tomb of the...</td>\n",
       "      <td>Answer: The Tomb of Alexander Stewart, also kn...</td>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Question: How long did the Siege of Mariupol l...</td>\n",
       "      <td>[The Siege of Mariupol began on 24 February 20...</td>\n",
       "      <td>Answer: The Siege of Mariupol began on 24 Febr...</td>\n",
       "      <td>0.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Question: What factors contributed to the Sri ...</td>\n",
       "      <td>[The Sri Lankan economic crisis is an ongoing ...</td>\n",
       "      <td>Answer: The Sri Lankan economic crisis was cau...</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Question: How many people were killed and inju...</td>\n",
       "      <td>[The 2022 Hormozgan earthquakes were a pair of...</td>\n",
       "      <td>Answer: The 2022 Hormozgan earthquakes were a ...</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Question: What was the estimated timeline for ...</td>\n",
       "      <td>[On December 3, 2022, a shooting attack was ca...</td>\n",
       "      <td>Answer: Initial estimates were that up to four...</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Question: What was the size and payload of the...</td>\n",
       "      <td>[From January 28 to February 4, 2023, a high-a...</td>\n",
       "      <td>Answer: The Chinese balloon that was spotted i...</td>\n",
       "      <td>0.057692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Question: When and where will the Miss Grand D...</td>\n",
       "      <td>[Miss Grand Dominican Republic 2023 will be th...</td>\n",
       "      <td>Answer: The Miss Grand Dominican Republic 2023...</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Question: What caused the gas supply outage in...</td>\n",
       "      <td>[The city of Sheffield, England was impacted b...</td>\n",
       "      <td>Answer: The gas supply outage in Sheffield, En...</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Question: What types of volcanoes are found on...</td>\n",
       "      <td>[The surface of Venus is dominated by volcanic...</td>\n",
       "      <td>Answer: On Venus, there are shield volcanoes, ...</td>\n",
       "      <td>0.107143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "11  Question: What is the Zubaydah Trail and when ...   \n",
       "29  Question: Who is buried in the Tomb of Alexand...   \n",
       "31  Question: How long did the Siege of Mariupol l...   \n",
       "33  Question: What factors contributed to the Sri ...   \n",
       "34  Question: How many people were killed and inju...   \n",
       "37  Question: What was the estimated timeline for ...   \n",
       "39  Question: What was the size and payload of the...   \n",
       "43  Question: When and where will the Miss Grand D...   \n",
       "45  Question: What caused the gas supply outage in...   \n",
       "47  Question: What types of volcanoes are found on...   \n",
       "\n",
       "                                             contexts  \\\n",
       "11  [The Zubaydah Trail (Al-Kufi pilgrimage route)...   \n",
       "29  [The Tomb of Alexander Stewart (or Tomb of the...   \n",
       "31  [The Siege of Mariupol began on 24 February 20...   \n",
       "33  [The Sri Lankan economic crisis is an ongoing ...   \n",
       "34  [The 2022 Hormozgan earthquakes were a pair of...   \n",
       "37  [On December 3, 2022, a shooting attack was ca...   \n",
       "39  [From January 28 to February 4, 2023, a high-a...   \n",
       "43  [Miss Grand Dominican Republic 2023 will be th...   \n",
       "45  [The city of Sheffield, England was impacted b...   \n",
       "47  [The surface of Venus is dominated by volcanic...   \n",
       "\n",
       "                                               answer  context_precision  \n",
       "11  Answer: The Zubaydah Trail, also known as the ...           0.428571  \n",
       "29  Answer: The Tomb of Alexander Stewart, also kn...           0.481481  \n",
       "31  Answer: The Siege of Mariupol began on 24 Febr...           0.042553  \n",
       "33  Answer: The Sri Lankan economic crisis was cau...           0.238095  \n",
       "34  Answer: The 2022 Hormozgan earthquakes were a ...           0.051282  \n",
       "37  Answer: Initial estimates were that up to four...           0.023256  \n",
       "39  Answer: The Chinese balloon that was spotted i...           0.057692  \n",
       "43  Answer: The Miss Grand Dominican Republic 2023...           0.058824  \n",
       "45  Answer: The gas supply outage in Sheffield, En...           0.125000  \n",
       "47  Answer: On Venus, there are shield volcanoes, ...           0.107143  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_good_df.iloc[indices.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0109c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Question: What is the Zubaydah Trail and when ...</td>\n",
       "      <td>[The Zubaydah Trail (Al-Kufi pilgrimage route)...</td>\n",
       "      <td>Answer: The Zubaydah Trail, also known as the ...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Question: Who is buried in the Tomb of Alexand...</td>\n",
       "      <td>[The Tomb of Alexander Stewart (or Tomb of the...</td>\n",
       "      <td>Answer: The Tomb of Alexander Stewart, also kn...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Question: How long did the Siege of Mariupol l...</td>\n",
       "      <td>[The Siege of Mariupol began on 24 February 20...</td>\n",
       "      <td>Answer: The Siege of Mariupol began on 24 Febr...</td>\n",
       "      <td>0.132075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Question: What factors contributed to the Sri ...</td>\n",
       "      <td>[The Sri Lankan economic crisis is an ongoing ...</td>\n",
       "      <td>Answer: The Sri Lankan economic crisis was cau...</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Question: How many people were killed and inju...</td>\n",
       "      <td>[The 2022 Hormozgan earthquakes were a pair of...</td>\n",
       "      <td>Answer: The 2022 Hormozgan earthquakes were a ...</td>\n",
       "      <td>0.057692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Question: What was the estimated timeline for ...</td>\n",
       "      <td>[On December 3, 2022, a shooting attack was ca...</td>\n",
       "      <td>Answer: Initial estimates were that up to four...</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Question: What was the size and payload of the...</td>\n",
       "      <td>[From January 28 to February 4, 2023, a high-a...</td>\n",
       "      <td>Answer: The Chinese balloon that was spotted i...</td>\n",
       "      <td>0.089286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Question: When and where will the Miss Grand D...</td>\n",
       "      <td>[Miss Grand Dominican Republic 2023 will be th...</td>\n",
       "      <td>Answer: The Miss Grand Dominican Republic 2023...</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Question: What caused the gas supply outage in...</td>\n",
       "      <td>[The city of Sheffield, England was impacted b...</td>\n",
       "      <td>Answer: The gas supply outage in Sheffield, En...</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Question: What types of volcanoes are found on...</td>\n",
       "      <td>[The surface of Venus is dominated by volcanic...</td>\n",
       "      <td>Answer: On Venus, there are shield volcanoes, ...</td>\n",
       "      <td>0.128571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "11  Question: What is the Zubaydah Trail and when ...   \n",
       "29  Question: Who is buried in the Tomb of Alexand...   \n",
       "31  Question: How long did the Siege of Mariupol l...   \n",
       "33  Question: What factors contributed to the Sri ...   \n",
       "34  Question: How many people were killed and inju...   \n",
       "37  Question: What was the estimated timeline for ...   \n",
       "39  Question: What was the size and payload of the...   \n",
       "43  Question: When and where will the Miss Grand D...   \n",
       "45  Question: What caused the gas supply outage in...   \n",
       "47  Question: What types of volcanoes are found on...   \n",
       "\n",
       "                                             contexts  \\\n",
       "11  [The Zubaydah Trail (Al-Kufi pilgrimage route)...   \n",
       "29  [The Tomb of Alexander Stewart (or Tomb of the...   \n",
       "31  [The Siege of Mariupol began on 24 February 20...   \n",
       "33  [The Sri Lankan economic crisis is an ongoing ...   \n",
       "34  [The 2022 Hormozgan earthquakes were a pair of...   \n",
       "37  [On December 3, 2022, a shooting attack was ca...   \n",
       "39  [From January 28 to February 4, 2023, a high-a...   \n",
       "43  [Miss Grand Dominican Republic 2023 will be th...   \n",
       "45  [The city of Sheffield, England was impacted b...   \n",
       "47  [The surface of Venus is dominated by volcanic...   \n",
       "\n",
       "                                               answer  context_precision  \n",
       "11  Answer: The Zubaydah Trail, also known as the ...           0.500000  \n",
       "29  Answer: The Tomb of Alexander Stewart, also kn...           0.500000  \n",
       "31  Answer: The Siege of Mariupol began on 24 Febr...           0.132075  \n",
       "33  Answer: The Sri Lankan economic crisis was cau...           0.266667  \n",
       "34  Answer: The 2022 Hormozgan earthquakes were a ...           0.057692  \n",
       "37  Answer: Initial estimates were that up to four...           0.074074  \n",
       "39  Answer: The Chinese balloon that was spotted i...           0.089286  \n",
       "43  Answer: The Miss Grand Dominican Republic 2023...           0.076923  \n",
       "45  Answer: The gas supply outage in Sheffield, En...           0.150000  \n",
       "47  Answer: On Venus, there are shield volcanoes, ...           0.128571  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bad_df.iloc[indices.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bca3e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.context_precision import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dace9a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94bbaf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(results_good_df.iloc[12]['contexts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de67061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The gas supply outage affected the Cadent Gas network, and was caused by a burst water main on the Yorkshire Water network which resulted in more than 2 million litres of water flooding into the gas supply network.\n",
    "The outage, which has predominantly affected the Hillsborough, Malin Bridge and Stannington districts, was declared a major incident by Sheffield City Council; some properties were without a gas supply for almost two weeks.\n",
    "The local authority, Sheffield City Council, officially declared a major incident on 7 December, with thousands of properties still without a gas supply and temperatures forecast to drop below freezing for prolonged periods in the coming days.\n",
    "According to Cadent Gas, around three-quarters of affected properties had been reconnected to the gas network by 10 December.\n",
    "Yorkshire Water issued an apology to affected customers on 6 December, stating that they \"understood how difficult it was\" for affected residents.\n",
    "Cadent Gas initially announced that residents would be compensated for any extra electricity usage caused by increased usage of portable electric heaters and other devices, before confirming that affected residents would receive double the usual rate of compensation for a gas outage, totalling £910 for a seven-day outage (or £1,470 for a commercial property).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f26c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74f0abbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01446ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
